---
title: "Week7 - Classification and Accuracy"
---

## Abstract

Here is the overview of remote sensing techniques for classifying and assessing the accuracy of land cover data. Here are the key points:

::: callout-tip

-   **Landcover Classification**: It discusses the use of pre-classified data sources like GlobeLand30, ESA’s CCI, Dynamic World, MODIS, and Google building data for landcover classification.

-   **Dynamic World**: The page details the process of training, pre-processing, normalization, and classification using CNNs, with a focus on Dynamic World’s semi-supervised approach and regional division for sample stratification.

-   **Sub Pixel Analysis**: It explains the concept of sub pixel classification, spectral mixture analysis, and linear spectral unmixing, including mathematical formulas for calculating the proportion of landcover per pixel.

-   **Accuracy Assessment**: The page outlines various accuracy assessment methods in remote sensing, such as producer’s accuracy, user’s accuracy, overall accuracy, and the Kappa coefficient, along with their definitions and significance.

:::



Building upon the foundation laid in Week 6, where I explored the classification process and tackled challenges like overfitting in decision trees, this week delves deeper into the evaluation of classification models, specifically focusing on **Accuracy Assessment** and the **Confusion Matrix**.

In the context of remote sensing and geospatial analysis, accuracy assessment is critical to understanding how well a classification model performs. A classification model assigns labels to pixels or objects in an image based on the training data provided. However, to gauge the model’s effectiveness, we need to compare the predicted labels with the actual ground truth labels, which is where the confusion matrix comes into play.

## Confusion Matrix and Accuracy Metrics

A confusion matrix is a table used to evaluate the performance of a classification algorithm. It provides a detailed breakdown of the model's predictions compared to the actual labels. The matrix consists of four key components:

- **True Positives (TP)**: The number of correct predictions where the model accurately classified a positive instance.
- **True Negatives (TN)**: The number of correct predictions where the model accurately classified a negative instance.
- **False Positives (FP)**: The number of incorrect predictions where the model incorrectly classified a negative instance as positive.
- **False Negatives (FN)**: The number of incorrect predictions where the model incorrectly classified a positive instance as negative.

These values are used to compute various metrics:

$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

$$\text{Precision} = \frac{TP}{TP + FP}$$

$$\text{Recall} = \frac{TP}{TP + FN}$$

$$\text{F1\ Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

These metrics provide insights into the model’s performance, indicating how well it can distinguish between different classes and how reliable its predictions are.

## Accuracy and Misclassification

While accuracy is a useful measure, it is important to be mindful of potential imbalances in the data, where one class may be more prevalent than others. In such cases, relying solely on accuracy can be misleading, as the model might simply be predicting the dominant class more frequently, leading to a high accuracy score but poor performance on minority classes. Therefore, other metrics like precision and recall, which focus on the model's ability to correctly identify positive instances, become crucial.

## Reflection

Continuing from the challenges discussed in Week 6, this week’s focus on accuracy and the confusion matrix highlights the importance of rigorous model evaluation in remote sensing. Ensuring that a classification model is not only accurate but also balanced and reliable across different classes is essential for effective geospatial analysis. This week has deepened my understanding of the critical role that accuracy assessment plays in model validation, and I am eager to apply these insights to improve the robustness of my classification models in future projects.


This reminds me of my undergraduate dissertation project, where I evaluated the accuracy of fire detection models using confusion matrices.

![Confusion Matrix](images/clipboard-2159447453.png){fig-align="left"}

**Firstly**, I prepared the data by creating binary classification labels using both active detection thresholding and visual inspection methods as ground truth data. **Next**, I trained the model using a U-net neural network model on remote sensing images, applying both cross-entropy and focal loss functions. **Finally**, I made predictions and evaluations by [predicting fire pixel locations]{.underline} in new images and [assessing the differences between expected and original images]{.underline} [using confusion matrix]{.underline} evaluation metrics to determine prediction accuracy. Evaluating accuracy was a crucial practical experience for me in using convolutional networks to predict fire spots in remote sensing images.
