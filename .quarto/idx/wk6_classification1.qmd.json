{"title":"Week6 - Classification-I","markdown":{"yaml":{"title":"Week6 - Classification-I"},"headingText":"Summary","containsRefs":false,"markdown":"\n\n\nIn this week, we explore the use of Google Earth Engine (GEE) for geospatial analysis. We start by loading administrative boundary data from the FAO GAUL global admin layers and proceed to work with Sentinel-2 surface reflectance data. The session covers cloud masking techniques, classification using supervised classifiers like Random Forest, and accuracy assessment. \n\n::: callout-tip\n\n*GEE Data Catalog*: The class discusses how to search and load data from the Google Earth Engine (GEE) data catalog, specifically focusing on FAO GAUL global admin layers and Sentinel data.\n\n*Script and Code Examples*: It provides example code for loading and filtering data, applying cloud masks, and handling Sentinel data.\n\n*Classification Process*: The page explains the process of classification using supervised classifiers like CART, RandomForest, NaiveBayes, and SVM, including steps for training and validating the model.\n\n*Training Data*: Instructions are given on how to select and use training data within the study area, including generating samples and merging polygons into a feature collection.\n\n:::\n\n## Classification Process\n\nClassification is a supervised learning technique used to categorize data points into predefined classes or categories based on input features. The goal is to learn a mapping from input variables to a set of discrete outcomes (labels) using a training dataset where the correct labels are known. \n\n### Principles of Classification(Sokal, R. R. , 1974[^wk6_classification1-1]; Zonneveld, I. S. , 1994[^wk6_classification1-2])\n\n1. **Training Phase**: During this phase, the classifier learns from labeled training data by identifying patterns and relationships between the input features and their corresponding labels. Popular algorithms for classification include decision trees, random forests, support vector machines, and neural networks.\n\n2. **Prediction Phase**: After training, the classifier is used to predict the class of new, unseen data points based on the patterns it has learned.\n\n3. **Evaluation**: The classifier's performance is evaluated using various metrics like accuracy, precision, recall, and the F1 score. Cross-validation is often employed to ensure the model generalizes well to new data.\n\n### Applications of Classification\n\nClassification is widely used across various domains, including:\n\n- **Medical Diagnosis**: To classify medical conditions based on symptoms or diagnostic test results. For example, using decision trees to predict the likelihood of a patient having a particular disease based on medical records.\n\n- **Spam Detection**: Email filtering systems classify emails as \"spam\" or \"not spam\" using algorithms like Naive Bayes(A. Almomani et al., 2013[^wk6_classification1-5]; Bahgat et al., 2016[^wk6_classification1-6]).\n\n![Taxonomy of email message structure](images/Taxonomy of email message structure.gif)\n\n![An example of message structure for the purpose of feature](images/Amessage structure.gif)\n\n- **Cing**: Financial institutions use classification models to assess the creditworthiness of applicants by predicting the probability of default .\n\n### Challenges: Overfitting\n\nOne of the significant challenges in the classification process is **overfitting**. Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise and outliers. As a result, the model performs exceptionally well on the training data but poorly on unseen data because it has become too complex and specific to the training examples.\n\n\n\n## Overfitting\n\n$$Tree score = SSR + TreePenalty_{alpha}* T_{Mumber Of Leaves}$$\n\nFor the Tree score formula, why does removing more leaves result in a larger alpha?\n\nDuring the pruning process of decision trees, we use a parameter called ccp_alpha (cost complexity parameter) to control the extent of pruning. A higher value of this parameter indicates a greater tendency to remove more leaf nodes, thereby simplifying the tree structure.\n\nLet's understand why removing more leaf nodes leads to a larger ccp_alpha:\n\n### Gini Impurity and Tree Complexity:\n\nGini impurity is a metric used to measure the purity of a dataset, with smaller values indicating higher purity.\n\nDuring pruning, we aim to retain leaf nodes that positively contribute to the model's performance while reducing the complexity of the model.\n\nWhen we remove a leaf node, the Gini impurity increases because we lose the purity associated with that leaf node.\n\n![Representation of a decision tree before and after being processed by a pruning algorithm, where the decision nodes (light blue) classify samples into 2 classes (green and red). The red dividing lines represent the pruning step.](images/clipboard-909472583.png){fig-align=\"center\"}\n\n### Cost Complexity:\n\nccp_alpha is a cost complexity parameter that balances the fit and complexity of the model during pruning.\n\nBy increasing ccp_alpha, the model tends to remove more leaf nodes, thereby reducing the complexity of the model.\n\nThe purpose of this is to prevent overfitting and improve the generalization ability of the model.\n\n## Reflection\n\nIn the field of remote sensing, these concepts pose several challenges and considerations that I find intriguing. Ensuring that the models I develop have strong generalization capabilities, capable of adapting to various environmental conditions, is paramount. Additionally, assessing and addressing data impurities arising from diverse factors in remote sensing datasets presents a fascinating puzzle to solve. Moreover, navigating the balance between constructing complex models to capture nuanced data variations while avoiding overfitting is a stimulating challenge that I look forward to tackling in this course.\n\n## Reference\n\n[^wk6_classification1-1]:Sokal, R. R. (1974). Classification: Purposes, Principles, Progress, Prospects: Clustering and other new techniques have changed classificatory principles and practice in many sciences. Science, 185(4157), 1115-1123.\n\n[^wk6_classification1-2]: Zonneveld, I. S. (1994). Basic principles of classification. In Ecosystem classification for environmental management (pp. 23-47). Dordrecht: Springer Netherlands.\n\n\n[^wk6_classification1-6]:Bahgat, E.M., Rady, S., Gad, W. (2016). An E-mail Filtering Approach Using Classification Techniques. In: Gaber, T., Hassanien, A., El-Bendary, N., Dey, N. (eds) The 1st International Conference on Advanced Intelligent System and Informatics (AISI2015), November 28-30, 2015, Beni Suef, Egypt. Advances in Intelligent Systems and Computing, vol 407. Springer, Cham. https://doi.org/10.1007/978-3-319-26690-9_29\n\n[^wk6_classification1-5]:A. Almomani, B. B. Gupta, S. Atawneh, A. Meulenberg and E. Almomani, \"A Survey of Phishing Email Filtering Techniques,\" in IEEE Communications Surveys & Tutorials, vol. 15, no. 4, pp. 2070-2090, Fourth Quarter 2013, doi: 10.1109/SURV.2013.030713.00020.\nkeywords: {Authentication;Machine learning;Electronic mail;Machine learning;Classification;Phishing;Phishing email;Filtering;Classifiers;Machine learning;Authentication;Network level protection},\n\n\n","srcMarkdownNoYaml":"\n\n## Summary\n\nIn this week, we explore the use of Google Earth Engine (GEE) for geospatial analysis. We start by loading administrative boundary data from the FAO GAUL global admin layers and proceed to work with Sentinel-2 surface reflectance data. The session covers cloud masking techniques, classification using supervised classifiers like Random Forest, and accuracy assessment. \n\n::: callout-tip\n\n*GEE Data Catalog*: The class discusses how to search and load data from the Google Earth Engine (GEE) data catalog, specifically focusing on FAO GAUL global admin layers and Sentinel data.\n\n*Script and Code Examples*: It provides example code for loading and filtering data, applying cloud masks, and handling Sentinel data.\n\n*Classification Process*: The page explains the process of classification using supervised classifiers like CART, RandomForest, NaiveBayes, and SVM, including steps for training and validating the model.\n\n*Training Data*: Instructions are given on how to select and use training data within the study area, including generating samples and merging polygons into a feature collection.\n\n:::\n\n## Classification Process\n\nClassification is a supervised learning technique used to categorize data points into predefined classes or categories based on input features. The goal is to learn a mapping from input variables to a set of discrete outcomes (labels) using a training dataset where the correct labels are known. \n\n### Principles of Classification(Sokal, R. R. , 1974[^wk6_classification1-1]; Zonneveld, I. S. , 1994[^wk6_classification1-2])\n\n1. **Training Phase**: During this phase, the classifier learns from labeled training data by identifying patterns and relationships between the input features and their corresponding labels. Popular algorithms for classification include decision trees, random forests, support vector machines, and neural networks.\n\n2. **Prediction Phase**: After training, the classifier is used to predict the class of new, unseen data points based on the patterns it has learned.\n\n3. **Evaluation**: The classifier's performance is evaluated using various metrics like accuracy, precision, recall, and the F1 score. Cross-validation is often employed to ensure the model generalizes well to new data.\n\n### Applications of Classification\n\nClassification is widely used across various domains, including:\n\n- **Medical Diagnosis**: To classify medical conditions based on symptoms or diagnostic test results. For example, using decision trees to predict the likelihood of a patient having a particular disease based on medical records.\n\n- **Spam Detection**: Email filtering systems classify emails as \"spam\" or \"not spam\" using algorithms like Naive Bayes(A. Almomani et al., 2013[^wk6_classification1-5]; Bahgat et al., 2016[^wk6_classification1-6]).\n\n![Taxonomy of email message structure](images/Taxonomy of email message structure.gif)\n\n![An example of message structure for the purpose of feature](images/Amessage structure.gif)\n\n- **Cing**: Financial institutions use classification models to assess the creditworthiness of applicants by predicting the probability of default .\n\n### Challenges: Overfitting\n\nOne of the significant challenges in the classification process is **overfitting**. Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise and outliers. As a result, the model performs exceptionally well on the training data but poorly on unseen data because it has become too complex and specific to the training examples.\n\n\n\n## Overfitting\n\n$$Tree score = SSR + TreePenalty_{alpha}* T_{Mumber Of Leaves}$$\n\nFor the Tree score formula, why does removing more leaves result in a larger alpha?\n\nDuring the pruning process of decision trees, we use a parameter called ccp_alpha (cost complexity parameter) to control the extent of pruning. A higher value of this parameter indicates a greater tendency to remove more leaf nodes, thereby simplifying the tree structure.\n\nLet's understand why removing more leaf nodes leads to a larger ccp_alpha:\n\n### Gini Impurity and Tree Complexity:\n\nGini impurity is a metric used to measure the purity of a dataset, with smaller values indicating higher purity.\n\nDuring pruning, we aim to retain leaf nodes that positively contribute to the model's performance while reducing the complexity of the model.\n\nWhen we remove a leaf node, the Gini impurity increases because we lose the purity associated with that leaf node.\n\n![Representation of a decision tree before and after being processed by a pruning algorithm, where the decision nodes (light blue) classify samples into 2 classes (green and red). The red dividing lines represent the pruning step.](images/clipboard-909472583.png){fig-align=\"center\"}\n\n### Cost Complexity:\n\nccp_alpha is a cost complexity parameter that balances the fit and complexity of the model during pruning.\n\nBy increasing ccp_alpha, the model tends to remove more leaf nodes, thereby reducing the complexity of the model.\n\nThe purpose of this is to prevent overfitting and improve the generalization ability of the model.\n\n## Reflection\n\nIn the field of remote sensing, these concepts pose several challenges and considerations that I find intriguing. Ensuring that the models I develop have strong generalization capabilities, capable of adapting to various environmental conditions, is paramount. Additionally, assessing and addressing data impurities arising from diverse factors in remote sensing datasets presents a fascinating puzzle to solve. Moreover, navigating the balance between constructing complex models to capture nuanced data variations while avoiding overfitting is a stimulating challenge that I look forward to tackling in this course.\n\n## Reference\n\n[^wk6_classification1-1]:Sokal, R. R. (1974). Classification: Purposes, Principles, Progress, Prospects: Clustering and other new techniques have changed classificatory principles and practice in many sciences. Science, 185(4157), 1115-1123.\n\n[^wk6_classification1-2]: Zonneveld, I. S. (1994). Basic principles of classification. In Ecosystem classification for environmental management (pp. 23-47). Dordrecht: Springer Netherlands.\n\n\n[^wk6_classification1-6]:Bahgat, E.M., Rady, S., Gad, W. (2016). An E-mail Filtering Approach Using Classification Techniques. In: Gaber, T., Hassanien, A., El-Bendary, N., Dey, N. (eds) The 1st International Conference on Advanced Intelligent System and Informatics (AISI2015), November 28-30, 2015, Beni Suef, Egypt. Advances in Intelligent Systems and Computing, vol 407. Springer, Cham. https://doi.org/10.1007/978-3-319-26690-9_29\n\n[^wk6_classification1-5]:A. Almomani, B. B. Gupta, S. Atawneh, A. Meulenberg and E. Almomani, \"A Survey of Phishing Email Filtering Techniques,\" in IEEE Communications Surveys & Tutorials, vol. 15, no. 4, pp. 2070-2090, Fourth Quarter 2013, doi: 10.1109/SURV.2013.030713.00020.\nkeywords: {Authentication;Machine learning;Electronic mail;Machine learning;Classification;Phishing;Phishing email;Filtering;Classifiers;Machine learning;Authentication;Network level protection},\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"wk6_classification1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["references.bib"],"editor":"visual","theme":"cosmo","darkmode":true,"title":"Week6 - Classification-I"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"wk6_classification1.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"editor":"visual","documentclass":"scrreprt","title":"Week6 - Classification-I"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}