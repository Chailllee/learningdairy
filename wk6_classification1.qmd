---
title: "Week6 - Classification-I"
---

## Summary

In this week, we explore the use of Google Earth Engine (GEE) for geospatial analysis. We start by loading administrative boundary data from the FAO GAUL global admin layers and proceed to work with Sentinel-2 surface reflectance data. The session covers cloud masking techniques, classification using supervised classifiers like Random Forest, and accuracy assessment. 

::: callout-tip

*GEE Data Catalog*: The page discusses how to search and load data from the Google Earth Engine (GEE) data catalog, specifically focusing on FAO GAUL global admin layers and Sentinel data.

*Script and Code Examples*: It provides example code for loading and filtering data, applying cloud masks, and handling Sentinel data.

*Classification Process*: The page explains the process of classification using supervised classifiers like CART, RandomForest, NaiveBayes, and SVM, including steps for training and validating the model.

*Training Data*: Instructions are given on how to select and use training data within the study area, including generating samples and merging polygons into a feature collection.

:::


## Overfitting

$$Tree score = SSR + TreePenalty_{alpha}* T_{Mumber Of Leaves}$$

For the Tree score formula, why does removing more leaves result in a larger alpha?

During the pruning process of decision trees, we use a parameter called ccp_alpha (cost complexity parameter) to control the extent of pruning. A higher value of this parameter indicates a greater tendency to remove more leaf nodes, thereby simplifying the tree structure.

Let's understand why removing more leaf nodes leads to a larger ccp_alpha:

### Gini Impurity and Tree Complexity:

Gini impurity is a metric used to measure the purity of a dataset, with smaller values indicating higher purity.

During pruning, we aim to retain leaf nodes that positively contribute to the model's performance while reducing the complexity of the model.

When we remove a leaf node, the Gini impurity increases because we lose the purity associated with that leaf node.

![Representation of a decision tree before and after being processed by a pruning algorithm, where the decision nodes (light blue) classify samples into 2 classes (green and red). The red dividing lines represent the pruning step.](images/clipboard-909472583.png){fig-align="center"}

### Cost Complexity:

ccp_alpha is a cost complexity parameter that balances the fit and complexity of the model during pruning.

By increasing ccp_alpha, the model tends to remove more leaf nodes, thereby reducing the complexity of the model.

The purpose of this is to prevent overfitting and improve the generalization ability of the model.

## Reflection

In the field of remote sensing, these concepts pose several challenges and considerations that I find intriguing. Ensuring that the models I develop have strong generalization capabilities, capable of adapting to various environmental conditions, is paramount. Additionally, assessing and addressing data impurities arising from diverse factors in remote sensing datasets presents a fascinating puzzle to solve. Moreover, navigating the balance between constructing complex models to capture nuanced data variations while avoiding overfitting is a stimulating challenge that I look forward to tackling in this course.
