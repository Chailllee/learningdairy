[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "learningDiary",
    "section": "",
    "text": "About me\nHello, my name is Xiaoyi Chen. I developed a keen interest in remote sensing during my undergraduate studies due to my involvement in a graduation project focused on remote sensing image processing. This experience sparked my curiosity and enthusiasm for exploring the intricacies of this field further.\nAdditionally, I have a passion for photography and enjoy capturing moments through my lens. Below are some of my photographic records."
  },
  {
    "objectID": "intro.html#beginning",
    "href": "intro.html#beginning",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.1 Beginning",
    "text": "1.1 Beginning\nBefore starting this course, I need to first understand the purpose of this course, which is to introduce the basic concepts and applications of remote sensing, and through this course of RS, the content we will learn includes:\n\nDefinition, classification and platform of remote perception\nInteraction of electromagnetic waves with the Earth’s surface\nFour resolutions for remote sensing data: spatial, spectral, temporal, and radiative\nFormat, selection, and limitations of remote sensing data\nPractical cases and analysis methods of remote perception data\n\nIt easily reminds me of the basic concepts of RS that I learned by myself during my undergraduate study. I believe that this course can not only help me review the past knowledge, but also fully supplement other knowledge."
  },
  {
    "objectID": "wk2_Portfolio.html",
    "href": "wk2_Portfolio.html",
    "title": "2  Presentation Ninja",
    "section": "",
    "text": "background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg)\n???\nImage credit: Wikimedia Commons\nclass: inverse, center, middle\n\n3 Get Started\n\n\n\n4 Hello World\nInstall the xaringan package from Github:\n\nremotes::install_github(\"yihui/xaringan\")\n\n–\nYou are recommended to use the RStudio IDE, but you do not have to.\n\nCreate a new R Markdown document from the menu File -&gt; New File -&gt; R Markdown -&gt; From Template -&gt; Ninja Presentation;1\n\n–\n\nClick the Knit button to compile it;\n\n–\n\nor use the RStudio Addin2 “Infinite Moon Reader” to live preview the slides (every time you update and save the Rmd document, the slides will be automatically reloaded in RStudio Viewer.\n\n.footnote[ [1] 中文用户请看这份教程\n[2] See #2 if you do not see the template or addin in RStudio. ]\n\n\n5 Hello Ninja\nAs a presentation ninja, you certainly should not be satisfied by the “Hello World” example. You need to understand more about two things:\n\nThe remark.js library;\nThe xaringan package;\n\nBasically xaringan injected the chakra of R Markdown (minus Pandoc) into remark.js. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (knitr).\n\n\n\n6 remark.js\nYou can see an introduction of remark.js from its homepage. You should read the remark.js Wiki at least once to know how to\n\ncreate a new slide (Markdown syntax* and slide properties);\nformat a slide (e.g. text alignment);\nconfigure the slideshow;\nand use the presentation (keyboard shortcuts).\n\nIt is important to be familiar with remark.js before you can understand the options in xaringan.\n.footnote[[*] It is different with Pandoc’s Markdown! It is limited but should be enough for presentation purposes. Come on… You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js may be improved in the future.]\nclass: inverse, middle, center\n\n\n7 Using xaringan\n\n\n\n8 xaringan\nProvides an R Markdown output format xaringan::moon_reader as a wrapper for remark.js, and you can use it in the YAML metadata, e.g.\n---\ntitle: \"A Cool Presentation\"\noutput:\n  xaringan::moon_reader:\n    yolo: true\n    nature:\n      autoplay: 30000\n---\nSee the help page ?xaringan::moon_reader for all possible options that you can use.\n\n\n\n9 remark.js vs xaringan\nSome differences between using remark.js (left) and using xaringan (right):\n.pull-left[ 1. Start with a boilerplate HTML file;\n\nPlain Markdown;\nWrite JavaScript to autoplay slides;\nManually configure MathJax;\nHighlight code with *;\nEdit Markdown source and refresh browser to see updated slides; ]\n\n.pull-right[ 1. Start with an R Markdown document;\n\nR Markdown (can embed R/other code chunks);\nProvide an option autoplay;\nMathJax just works;*\nHighlight code with {{}};\nThe RStudio addin “Infinite Moon Reader” automatically refreshes slides on changes; ]\n\n.footnote[[*] Not really. See next page.]\n\n\n\n10 Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $+$ renders \\(\\alpha+\\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $.\nMath does not work on the title slide (see #61 for a workaround).\n\n\n\n\n11 R Code\n\n# a boring regression\nfit = lm(dist ~ 1 + speed, data = cars)\ncoef(summary(fit))\n\n#               Estimate Std. Error   t value     Pr(&gt;|t|)\n# (Intercept) -17.579095  6.7584402 -2.601058 1.231882e-02\n# speed         3.932409  0.4155128  9.463990 1.489836e-12\n\ndojutsu = c('地爆天星', '天照', '加具土命', '神威', '須佐能乎', '無限月読')\ngrep('天', dojutsu, value = TRUE)\n\n# [1] \"地爆天星\" \"天照\"\n\n\n\n\n\n12 R Plots\n\npar(mar = c(4, 4, 1, .1))\nplot(cars, pch = 19, col = 'darkgray', las = 1)\nabline(fit, lwd = 2)\n\n\n\n\n\n\n\n13 Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\nknitr::kable(head(iris), format = 'html')\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\n\n\n\n14 HTML Widgets\nI have not thoroughly tested HTML widgets against xaringan. Some may work well, and some may not. It is a little tricky.\nSimilarly, the Shiny mode (runtime: shiny) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications.\nSee the next page for two HTML widgets.\n\n\nlibrary(leaflet)\nleaflet() %&gt;% addTiles() %&gt;% setView(-93.65, 42.0285, zoom = 17)\n\n\n\n\n\n\n\nDT::datatable(\n  head(iris, 10),\n  fillContainer = FALSE, options = list(pageLength = 8)\n)\n\n\n\n\n15 Some Tips\n\nDo not forget to try the yolo option of xaringan::moon_reader.\noutput:\n  xaringan::moon_reader:\n    yolo: true\n\n\n\n\n16 Some Tips\n\nSlides can be automatically played if you set the autoplay option under nature, e.g. go to the next slide every 30 seconds in a lightning talk:\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay: 30000\nIf you want to restart the play after it reaches the last slide, you may set the sub-option loop to TRUE, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay:\n        interval: 30000\n        loop: true\n\n\n\n\n17 Some Tips\n\nA countdown timer can be added to every page of the slides using the countdown option under nature, e.g. if you want to spend one minute on every page when you give the talk, you can set:\noutput:\n  xaringan::moon_reader:\n    nature:\n      countdown: 60000\nThen you will see a timer counting down from 01:00, to 00:59, 00:58, … When the time is out, the timer will continue but the time turns red.\n\n\n\n\n18 Some Tips\n\nThe title slide is created automatically by xaringan, but it is just another remark.js slide added before your other slides.\nThe title slide is set to class: center, middle, inverse, title-slide by default. You can change the classes applied to the title slide with the titleSlideClass option of nature (title-slide is always applied).\noutput:\n  xaringan::moon_reader:\n    nature:\n      titleSlideClass: [top, left, inverse]\n\n–\n\nIf you’d like to create your own title slide, disable xaringan’s title slide with the seal = FALSE option of moon_reader.\noutput:\n  xaringan::moon_reader:\n    seal: false\n\n\n\n\n19 Some Tips\n\nThere are several ways to build incremental slides. See this presentation for examples.\nThe option highlightLines: true of nature will highlight code lines that start with *, or are wrapped in {{ }}, or have trailing comments #&lt;&lt;;\noutput:\n  xaringan::moon_reader:\n    nature:\n      highlightLines: true\nSee examples on the next page.\n\n\n\n\n20 Some Tips\n.pull-left[ An example using a leading *:\n```r\nif (TRUE) {\n** message(\"Very important!\")\n}\n```\nOutput:\nif (TRUE) {\n* message(\"Very important!\")\n}\nThis is invalid R code, so it is a plain fenced code block that is not executed. ]\n.pull-right[ An example using {{}}:\n```{r tidy=FALSE}\nif (TRUE) {\n*{{ message(\"Very important!\") }}\n}\n```\nOutput:\n\nif (TRUE) {\n{{ message(\"Very important!\") }}\n}\n\nVery important!\n\n\nIt is valid R code so you can run it. Note that {{}} can wrap an R expression of multiple lines. ]\n\n\n\n21 Some Tips\nAn example of using the trailing comment #&lt;&lt; to highlight lines:\n```{r tidy=FALSE}\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #&lt;&lt;\n  geom_smooth()    #&lt;&lt;\n```\nOutput:\n\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #&lt;&lt;\n  geom_smooth()    #&lt;&lt;\n\n\n\n\n22 Some Tips\nWhen you enable line-highlighting, you can also use the chunk option highlight.output to highlight specific lines of the text output from a code chunk. For example, highlight.output = TRUE means highlighting all lines, and highlight.output = c(1, 3) means highlighting the first and third line.\n```{r, highlight.output=c(1, 3)}\nhead(iris)\n```\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nQuestion: what does highlight.output = c(TRUE, FALSE) mean? (Hint: think about R’s recycling of vectors)\n\n\n\n23 Some Tips\n\nTo make slides work offline, you need to download a copy of remark.js in advance, because xaringan uses the online version by default (see the help page ?xaringan::moon_reader).\nYou can use xaringan::summon_remark() to download the latest or a specified version of remark.js. By default, it is downloaded to libs/remark-latest.min.js.\nThen change the chakra option in YAML to point to this file, e.g.\noutput:\n  xaringan::moon_reader:\n    chakra: libs/remark-latest.min.js\nIf you used Google fonts in slides (the default theme uses Yanone Kaffeesatz, Droid Serif, and Source Code Pro), they won’t work offline unless you download or install them locally. The Heroku app google-webfonts-helper can help you download fonts and generate the necessary CSS.\n\n\n\n\n24 Macros\n\nremark.js allows users to define custom macros (JS functions) that can be applied to Markdown text using the syntax ![:macroName arg1, arg2, ...] or ![:macroName arg1, arg2, ...](this). For example, before remark.js initializes the slides, you can define a macro named scale:\nremark.macros.scale = function (percentage) {\n  var url = this;\n  return '&lt;img src=\"' + url + '\" style=\"width: ' + percentage + '\" /&gt;';\n};\nThen the Markdown text\n![:scale 50%](image.jpg)\nwill be translated to\n&lt;img src=\"image.jpg\" style=\"width: 50%\" /&gt;\n\n\n\n\n25 Macros (continued)\n\nTo insert macros in xaringan slides, you can use the option beforeInit under the option nature, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      beforeInit: \"macros.js\"\nYou save your remark.js macros in the file macros.js.\nThe beforeInit option can be used to insert arbitrary JS code before remark.create(). Inserting macros is just one of its possible applications.\n\n\n\n\n26 CSS\nAmong all options in xaringan::moon_reader, the most challenging but perhaps also the most rewarding one is css, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know.\nYou can see the default CSS file here. You can completely replace it with your own CSS files, or define new rules to override the default. See the help page ?xaringan::moon_reader for more information.\n\n\n\n27 CSS\nFor example, suppose you want to change the font for code from the default “Source Code Pro” to “Ubuntu Mono”. You can create a CSS file named, say, ubuntu-mono.css:\n@import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);\n\n.remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }\nThen set the css option in the YAML metadata:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"ubuntu-mono.css\"]\nHere I assume ubuntu-mono.css is under the same directory as your Rmd.\nSee yihui/xaringan#83 for an example of using the Fira Code font, which supports ligatures in program code.\n\n\n\n28 CSS (with Sass)\nxaringan also supports Sass support via rmarkdown. Suppose you want to use the same color for different elements, e.g., first heading and bold text. You can create a .scss file, say mytheme.scss, using the sass syntax with variables:\n$mycolor: #ff0000; \n.remark-slide-content &gt; h1 { color: $mycolor; }\n.remark-slide-content strong { color: $mycolor; }\nThen set the css option in the YAML metadata using this file placed under the same directory as your Rmd:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"mytheme.scss\"]\nThis requires rmarkdown &gt;= 2.8 and the sass package. You can learn more about rmarkdown and sass support in this blog post and in sass overview vignette.\n\n\n\n29 Themes\nDon’t want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files foo.css and foo-fonts.css, where foo is the theme name. Below are some existing themes:\n\nnames(xaringan:::list_css())\n\n [1] \"chocolate-fonts\"  \"chocolate\"        \"default-fonts\"   \n [4] \"default\"          \"duke-blue\"        \"fc-fonts\"        \n [7] \"fc\"               \"glasgow_template\" \"hygge-duke\"      \n[10] \"hygge\"            \"ki-fonts\"         \"ki\"              \n[13] \"kunoichi\"         \"lucy-fonts\"       \"lucy\"            \n[16] \"metropolis-fonts\" \"metropolis\"       \"middlebury-fonts\"\n[19] \"middlebury\"       \"nhsr-fonts\"       \"nhsr\"            \n[22] \"ninjutsu\"         \"rladies-fonts\"    \"rladies\"         \n[25] \"robot-fonts\"      \"robot\"            \"rutgers-fonts\"   \n[28] \"rutgers\"          \"shinobi\"          \"tamu-fonts\"      \n[31] \"tamu\"             \"uio-fonts\"        \"uio\"             \n[34] \"uo-fonts\"         \"uo\"               \"uol-fonts\"       \n[37] \"uol\"              \"useR-fonts\"       \"useR\"            \n[40] \"uwm-fonts\"        \"uwm\"              \"wic-fonts\"       \n[43] \"wic\"             \n\n\n\n\n\n30 Themes\nTo use a theme, you can specify the css option as an array of CSS filenames (without the .css extensions), e.g.,\noutput:\n  xaringan::moon_reader:\n    css: [default, metropolis, metropolis-fonts]\nIf you want to contribute a theme to xaringan, please read this blog post.\nbackground-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) background-size: 100px background-position: 90% 8%\n\n\n31 Sharingan\nThe R package name xaringan was derived1 from Sharingan, a dōjutsu in the Japanese anime Naruto with two abilities:\n\nthe “Eye of Insight”\nthe “Eye of Hypnotism”\n\nI think a presentation is basically a way to communicate insights to the audience, and a great presentation may even “hypnotize” the audience.2,3\n.footnote[ [1] In Chinese, the pronounciation of X is Sh /ʃ/ (as in shrimp). Now you should have a better idea of how to pronounce my last name Xie.\n[2] By comparison, bad presentations only put the audience to sleep.\n[3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations. ]\n\n\n\n32 Naruto terminology\nThe xaringan package borrowed a few terms from Naruto, such as\n\nSharingan (写輪眼; the package name)\nThe moon reader (月読; an attractive R Markdown output format)\nChakra (查克拉; the path to the remark.js library, which is the power to drive the presentation)\nNature transformation (性質変化; transform the chakra by setting different options)\nThe infinite moon reader (無限月読; start a local web server to continuously serve your slides)\nThe summoning technique (download remark.js from the web)\n\nYou can click the links to know more about them if you want. The jutsu “Moon Reader” may seem a little evil, but that does not mean your slides are evil.\n\nclass: center\n\n\n33 Hand seals (印)\nPress h or ? to see the possible ninjutsu you can use in remark.js.\n\n\nclass: center, middle\n\n\n34 Thanks!\nSlides created via the R package xaringan.\nThe chakra comes from remark.js, knitr, and R Markdown."
  },
  {
    "objectID": "wk5_GEE.html#summary",
    "href": "wk5_GEE.html#summary",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThe theoretical part of the course mainly introduced the basic concepts, data structures, operations, and application cases of Google Earth Engine (GEE).\n\n\n\n\n\n\nTip\n\n\n\nCharacteristics of GEE: GEE is a geospatial processing service that utilizes cloud servers to store and analyze massive amounts of remote sensing imagery and geographic data, enabling rapid and large-scale monitoring and simulation of changes on the Earth’s surface.\nData structure of GEE: Data in GEE is divided into two types: Images and Features, corresponding to raster and vector data, respectively. Images and Features can form Collections, representing stacks of multiple images or features. Data in GEE exists in the form of objects, each with its own properties and methods.\nOperations in GEE: GEE uses the JavaScript language for coding, which can be run in the browser. The code in GEE is divided into client-side and server-side, with the client-side code mainly used for interface control and interaction, and the server-side code mainly used for data processing and analysis. Data processing in GEE mainly relies on reducers, which can perform various statistical, analytical, and transformation operations on images or features. GEE also provides some advanced features such as regression, joining, and machine learning."
  },
  {
    "objectID": "wk5_GEE.html#whats-gee",
    "href": "wk5_GEE.html#whats-gee",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.2 What’s GEE",
    "text": "5.2 What’s GEE"
  },
  {
    "objectID": "wk7_classification1.html#overfiting",
    "href": "wk7_classification1.html#overfiting",
    "title": "4  Week7 - Classification-I",
    "section": "4.1 Overfiting",
    "text": "4.1 Overfiting\n\nTree score = SSR + tree penalty (alpha) * T (number of leaves)\n\n对于Tree score公式，为啥去掉越多leaf, alpha越大？\n在决策树的剪枝过程中，我们使用一个参数称为ccp_alpha（cost complexity parameter）来控制剪枝的程度。这个参数的值越大，意味着我们更倾向于剪掉更多的叶节点，从而简化树的结构。\n让我们来理解一下为什么去掉越多的叶节点，ccp_alpha 就越大：\n\n基尼杂质和树的复杂度：\n\n基尼杂质是用来衡量数据集纯度的指标，它越小表示数据集的纯度越高。\n在剪枝过程中，我们希望保留那些对模型性能有积极影响的叶节点，同时减少模型的复杂度。\n当我们去掉一个叶节点时，基尼杂质会增加，因为我们丧失了这个叶节点的纯度。\n\n成本复杂度：\n\nccp_alpha 是一个成本复杂度参数，它在剪枝过程中平衡了模型的拟合程度和复杂度。\n当我们增大 ccp_alpha 时，模型更倾向于剪掉更多的叶节点，从而减少模型的复杂度。\n这样做的目的是防止过拟合，提高模型的泛化能力。\nYoutube video\nHow to Prune Regression Trees"
  },
  {
    "objectID": "wk7_classification1.html#random-forest",
    "href": "wk7_classification1.html#random-forest",
    "title": "4  Week7 - Classification-I",
    "section": "4.2 Random Forest",
    "text": "4.2 Random Forest"
  },
  {
    "objectID": "wk9_SAR_data.html#abstract",
    "href": "wk9_SAR_data.html#abstract",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.1 Abstract",
    "text": "8.1 Abstract\nThis class introduces the basic principle of synthetic aperture radar (SAR), two sensor types, SAR data background, SAR data value, SAR polarization, SAR background, differential interferometric synthetic aperture radar (DlnSAR), SAR data processing, SAR data fusion, SAR image fusion and other concepts. This paper also introduces how to use SAR data for change detection, including statistical test, threshold screening and ROC curve, image fusion and so on. Finally, a change detection algorithm based on SAR data is proposed, and an example is given."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "9  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "intro.html#summary-of-week-1-class",
    "href": "intro.html#summary-of-week-1-class",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.2 Summary of week 1 class",
    "text": "1.2 Summary of week 1 class\n\n远程感知城市和环境：这是一门介绍远程感知基本原理和应用的课程，主要关注城市和环境问题。\n远程感知的类型和原理：远程感知分为主动和被动两种，根据是否有自身的能源发射器。远程感知数据受到电磁波与大气和地表的相互作用的影响，需要进行校正和处理。\n远程感知数据的格式和分辨率：远程感知数据通常是栅格数据，有不同的存储格式和组织方式。远程感知数据的质量和应用受到空间、光谱、时间和辐射分辨率的制约。"
  },
  {
    "objectID": "intro.html#reflection",
    "href": "intro.html#reflection",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.4 Reflection",
    "text": "1.4 Reflection\nThrough the basic knowledge of this class, I inadvertently deepened my understanding of Landsat series satellites from the concepts of electromagnetic wave, multispectral image and hyperspectral image. Landsat provides the longest continuous space record of Earth’s land mass. Its data is essential for us to make informed decisions about the Earth’s resources and environment. It Landsat is more than just a camera circling the globe with an excellent zoom lens. It measures how much light the Earth reflects from the sun. With Landsat, we have access to a variety of useful images that reveal more about the Earth and help us better understand and manage our planet. For example, these images can be used to monitor the effects of natural and human factors such as land cover, climate change, urbanization, drought, fires, changes in biomass, etc. Therefore, the public use of Landsat data makes a lot of sense, and experts, scholars, and enterprise engineers can implement projects according to their needs."
  },
  {
    "objectID": "intro.html#summary-of-this-class",
    "href": "intro.html#summary-of-this-class",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.1 Summary of this class",
    "text": "1.1 Summary of this class\n\n\n\n\n\n\nSummary\n\n\n\n\nTele-sensing Cities and Environments: This is a course that introduces the basic principles and applications of tele-sensing, with a focus on urban and environmental issues.\nTypes and principles of remote sensing: Remote sensing is divided into active and passive, depending on whether it has its own energy transmitter. Remote sensing data is affected by the interaction of electromagnetic waves with the atmosphere and the surface, and needs to be corrected and processed.\nFormat and resolution of remote sensing data: Remote sensing data is usually raster data and has different storage formats and ways of organizing. The quality and application of remote sensing data are constrained by spatial, spectral, temporal and radiative resolution.\n\n\n\nIt easily reminds me of the basic concepts of RS that I learned by myself during my undergraduate study. I believe that this course can not only help me review the past knowledge, but also fully supplement other knowledge."
  },
  {
    "objectID": "intro.html#遥感的基本概念与原理",
    "href": "intro.html#遥感的基本概念与原理",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.3 遥感的基本概念与原理",
    "text": "1.3 遥感的基本概念与原理\n\n1.3.1 被动传感器与主动传感器的区别\n两者的区别在于：被动传感器受大气散射的影响，需要在光照条件良好的时候工作，而主动传感器可以穿透云层、火山灰和大气条件，可以在夜间工作。\n我能找到的关于主动传感器的例子是合成孔径雷达（SAR），可以“看穿云层”，并且具有极化特性，可以根据表面的粗糙度、形状、方向、湿度、盐度、密度等反映出不同的电磁波。\n\n\n1.3.2 关于电磁波"
  },
  {
    "objectID": "wk9_SAR_data.html#difference-of-other-sar",
    "href": "wk9_SAR_data.html#difference-of-other-sar",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.2 Difference of other SAR",
    "text": "8.2 Difference of other SAR\n\n\n\n\n\n\n\n\n\nSAR\nInSAR\nDInSAR\nPSInSAR\n\n\n\n\nSynthetic Aperture Radar\nInterferometric Synthetic Aperture Radar\nDifferential Interferometric Synthetic Aperture Radar\nPersistent Scatterer Interferometric SAR\n\n\n\nA radar system capable of producing high-resolution images.\nUses a “virtual” antenna length to combine echo signals received from different positions, resulting in higher-resolution radar imaging.\nUsed for surface observations such as land use, topography, and forest cover.\n\n\nAnalyzes surface deformation by exploiting the phase difference between two remote sensing images.\nCalculates the deformation at each pixel on the ground surface between two observations.\nReveals elevation and deformation information.\n\n\nBuilds upon InSAR by using phase differences from multiple remote sensing images to improve deformation measurement accuracy.\nSensitive to deformation, suitable for monitoring ground surface changes due to earthquakes, mining, landslides, etc.\nUtilizes two SAR images and external topographic data to measure subtle surface deformations.\n\n\nModels and analyzes time series data from multiple SAR images to enhance deformation inversion accuracy.\nReveals spatial distribution of surface deformations, widely used for monitoring urban subsidence and infrastructure changes."
  },
  {
    "objectID": "wk9_SAR_data.html#sar数据的应用",
    "href": "wk9_SAR_data.html#sar数据的应用",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.3 SAR数据的应用",
    "text": "8.3 SAR数据的应用\n分析两个图像之间的变化（例如比率或对数比率）\nOpen Access Damage Detection Using Sentinel-1 Imagery\nBlast Damage Assessment\n通过以下方式查看随时间变化的差异：\nPixel-Wise T-Test"
  },
  {
    "objectID": "intro.html#basic-concepts-and-principles-of-remote-sensing",
    "href": "intro.html#basic-concepts-and-principles-of-remote-sensing",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.2 Basic concepts and principles of remote sensing",
    "text": "1.2 Basic concepts and principles of remote sensing\n\n1.2.1 The difference between passive and active sensors\nThe difference between the two is that passive sensors are affected by atmospheric scattering and need to work when light conditions are good, while active sensors can penetrate clouds, volcanic ash and atmospheric conditions and can work at night.\n\n\n\nActive remote sensing example\nPassive remote sensing example\n\n\n\n\nFor the untrained eye, it’s just a bunch of black and white pixels. But the reality is that there’s more than meets the eye. For example, the 3 main types of backscatter are: Specular reflection Double-bounce Diffuse scattering\nPassive remote sensing can be very similar to how our eyes interpret the world. But the power of passive remote sensing is to see light in the whole electromagnetic spectrum. For example, this multispectral image can have different band combinations like color infrared.\n\n\nAn example of an active sensor is synthetic Aperture radar (SAR), which can “see through the clouds” and has polarization characteristics that can reflect different electromagnetic waves based on surface roughness, shape, orientation, humidity, salinity, density, etc.\nIn terms of passive remote sensing, the Landsat mission is the longest-running earth observation program. On board Landsat-8, OLI generates 9 spectral bands (Band 1 to 9).\n\n\n\n\na SAR image\n\nRocky Mountains in true color\n\n\n\n\n\n1.2.2 The relation between electromagnetic wave and multispectrum\nAccording to the frequency of electromagnetic wave vibration, the electromagnetic spectrum can be divided into visible spectrum and invisible electromagnetic spectrum two parts.\nMultispectral remote sensing refers to the remote sensing observation and research of ground targets in multiple spectral bands. These bands can include infrared, visible, near-infrared, etc. By analyzing the information of these different bands, more information about the ground target can be obtained.\n\n\n\nMultispectral common channels\n\n\n\n\n\nComparison of spectral bands between Sentinel-2 and Landsat-81.\n\n\nIn general, the relationship between electromagnetic wave and multi-spectrum is that multi-spectral remote sensing uses different bands of electromagnetic wave, and more information of ground targets can be obtained by analyzing the information of these bands."
  },
  {
    "objectID": "intro.html#reference",
    "href": "intro.html#reference",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.5 Reference",
    "text": "1.5 Reference"
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Week1 - Introduction of RS",
    "section": "",
    "text": "López-Puigdollers D, Mateo-García G, Gómez-Chova L. Benchmarking Deep Learning Models for Cloud Detection in Landsat-8 and Sentinel-2 Images. Remote Sensing. 2021; 13(5):992. https://doi.org/10.3390/rs13050992↩︎\nSingh, D., Dohal, S., Yadav, G., Pandey, H., & Singh, V. (2023). Utilizing GIS and remote sensing methods for polluted water body remediation: A deeper understanding. Sustainable Systems Science: Geospatial Methods and Modeling (Vol. 1, pp. 155-169). https://doi.org/10.1016/B978-0-323-91880-0.00021-0↩︎\nYu, H., Kong, B., Wang, Q., Liu, X. & Liu, X. (2020). Hyperspectral remote sensing applications in soil: a review1. In Earth Observation, (pp. 269-291). DOI: &lt;10.1016/B978-0-08-102894-0.00011-5&gt;↩︎"
  },
  {
    "objectID": "wk2_xaringen.html",
    "href": "wk2_xaringen.html",
    "title": "2  Week 2 - Xaringen",
    "section": "",
    "text": "3 Get Started\nInstall the xaringan package from Github:\n{r eval=FALSE, tidy=FALSE} remotes::install_github(\"yihui/xaringan\")\n–\nYou are recommended to use the RStudio IDE, but you do not have to.\n–\n–\n.footnote[ [1] 中文用户请看这份教程\n[2] See #2 if you do not see the template or addin in RStudio. ]\nAs a presentation ninja, you certainly should not be satisfied by the “Hello World” example. You need to understand more about two things:\nBasically xaringan injected the chakra of R Markdown (minus Pandoc) into remark.js. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (knitr).\nYou can see an introduction of remark.js from its homepage. You should read the remark.js Wiki at least once to know how to\nIt is important to be familiar with remark.js before you can understand the options in xaringan.\n.footnote[[*] It is different with Pandoc’s Markdown! It is limited but should be enough for presentation purposes. Come on… You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js may be improved in the future.]\nclass: inverse, middle, center\nProvides an R Markdown output format xaringan::moon_reader as a wrapper for remark.js, and you can use it in the YAML metadata, e.g.\nSee the help page ?xaringan::moon_reader for all possible options that you can use.\nSome differences between using remark.js (left) and using xaringan (right):\n.pull-left[ 1. Start with a boilerplate HTML file;\n.pull-right[ 1. Start with an R Markdown document;\n.footnote[[*] Not really. See next page.]\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $+$ renders \\(\\alpha+\\beta\\). You can use the display style with double dollar signs:\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nLimitations:\n{r comment='#'} # a boring regression fit = lm(dist ~ 1 + speed, data = cars) coef(summary(fit)) dojutsu = c('地爆天星', '天照', '加具土命', '神威', '須佐能乎', '無限月読') grep('天', dojutsu, value = TRUE)\n{r cars, fig.height=4, dev='svg'} par(mar = c(4, 4, 1, .1)) plot(cars, pch = 19, col = 'darkgray', las = 1) abline(fit, lwd = 2)\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\nI have not thoroughly tested HTML widgets against xaringan. Some may work well, and some may not. It is a little tricky.\nSimilarly, the Shiny mode (runtime: shiny) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications.\nSee the next page for two HTML widgets.\n{r out.width='100%', fig.height=6, eval=require('leaflet')} library(leaflet) leaflet() %&gt;% addTiles() %&gt;% setView(-93.65, 42.0285, zoom = 17)\n{r eval=require('DT'), tidy=FALSE} DT::datatable(   head(iris, 10),   fillContainer = FALSE, options = list(pageLength = 8) )\n–\n.pull-left[ An example using a leading *:\nOutput:\nThis is invalid R code, so it is a plain fenced code block that is not executed. ]\n.pull-right[ An example using {{}}:\nOutput:\n{r tidy=FALSE} if (TRUE) { {{ message(\"Very important!\") }} }\nIt is valid R code so you can run it. Note that {{}} can wrap an R expression of multiple lines. ]\nAn example of using the trailing comment #&lt;&lt; to highlight lines:\nOutput:\n{r tidy=FALSE, eval=FALSE} library(ggplot2) ggplot(mtcars) +    aes(mpg, disp) +    geom_point() +   #&lt;&lt;   geom_smooth()    #&lt;&lt;\nWhen you enable line-highlighting, you can also use the chunk option highlight.output to highlight specific lines of the text output from a code chunk. For example, highlight.output = TRUE means highlighting all lines, and highlight.output = c(1, 3) means highlighting the first and third line.\n{r, highlight.output=c(1, 3), echo=FALSE} head(iris)\nQuestion: what does highlight.output = c(TRUE, FALSE) mean? (Hint: think about R’s recycling of vectors)\nAmong all options in xaringan::moon_reader, the most challenging but perhaps also the most rewarding one is css, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know.\nYou can see the default CSS file here. You can completely replace it with your own CSS files, or define new rules to override the default. See the help page ?xaringan::moon_reader for more information.\nFor example, suppose you want to change the font for code from the default “Source Code Pro” to “Ubuntu Mono”. You can create a CSS file named, say, ubuntu-mono.css:\nThen set the css option in the YAML metadata:\nHere I assume ubuntu-mono.css is under the same directory as your Rmd.\nSee yihui/xaringan#83 for an example of using the Fira Code font, which supports ligatures in program code.\nxaringan also supports Sass support via rmarkdown. Suppose you want to use the same color for different elements, e.g., first heading and bold text. You can create a .scss file, say mytheme.scss, using the sass syntax with variables:\nThen set the css option in the YAML metadata using this file placed under the same directory as your Rmd:\nThis requires rmarkdown &gt;= 2.8 and the sass package. You can learn more about rmarkdown and sass support in this blog post and in sass overview vignette.\nDon’t want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files foo.css and foo-fonts.css, where foo is the theme name. Below are some existing themes:\n{r, R.options=list(width = 70)} names(xaringan:::list_css())\nTo use a theme, you can specify the css option as an array of CSS filenames (without the .css extensions), e.g.,\nIf you want to contribute a theme to xaringan, please read this blog post.\nbackground-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) background-size: 100px background-position: 90% 8%\nThe R package name xaringan was derived1 from Sharingan, a dōjutsu in the Japanese anime Naruto with two abilities:\nI think a presentation is basically a way to communicate insights to the audience, and a great presentation may even “hypnotize” the audience.2,3\n.footnote[ [1] In Chinese, the pronounciation of X is Sh /ʃ/ (as in shrimp). Now you should have a better idea of how to pronounce my last name Xie.\n[2] By comparison, bad presentations only put the audience to sleep.\n[3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations. ]\nThe xaringan package borrowed a few terms from Naruto, such as\nYou can click the links to know more about them if you want. The jutsu “Moon Reader” may seem a little evil, but that does not mean your slides are evil.\nclass: center\nPress h or ? to see the possible ninjutsu you can use in remark.js.\nclass: center, middle\nSlides created via the R package xaringan.\nThe chakra comes from remark.js, knitr, and R Markdown. ```"
  },
  {
    "objectID": "wk2_xaringen.html#abstract",
    "href": "wk2_xaringen.html#abstract",
    "title": "2  Week 2 - Xaringen",
    "section": "2.1 Abstract",
    "text": "2.1 Abstract"
  },
  {
    "objectID": "wk3_RS_data.html#abstract",
    "href": "wk3_RS_data.html#abstract",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.1 Abstract",
    "text": "3.1 Abstract\n\n\n\n\n\n\nRemote Sensing Data\n\n\n\nThis lecture mainly provides detailed information on remote sensing cities and environments, especially on the use and processing of remote sensing data, and introduces the correction methods of remote sensing data, including geometric, atmospheric, orthographic/topographic correction and radiometric correction. Prior to this, data processing is required, including data linking and enhancement such as feathering, image enhancement, and other enhancement techniques. In addition to this, this lesson introduces remote sensing technology, explaining the different types of remote sensing sensors, such as push-sweep and scan sensors, and how they work."
  },
  {
    "objectID": "wk4_policy.html#abstract",
    "href": "wk4_policy.html#abstract",
    "title": "4  Week4 - Policy Applications",
    "section": "4.1 Abstract",
    "text": "4.1 Abstract"
  },
  {
    "objectID": "wk6_classification1.html#overfiting",
    "href": "wk6_classification1.html#overfiting",
    "title": "6  Week7 - Classification-I",
    "section": "6.1 Overfiting",
    "text": "6.1 Overfiting\n\nTree score = SSR + tree penalty (alpha) * T (number of leaves)\n\n对于Tree score公式，为啥去掉越多leaf, alpha越大？\n在决策树的剪枝过程中，我们使用一个参数称为ccp_alpha（cost complexity parameter）来控制剪枝的程度。这个参数的值越大，意味着我们更倾向于剪掉更多的叶节点，从而简化树的结构。\n让我们来理解一下为什么去掉越多的叶节点，ccp_alpha 就越大：\n\n基尼杂质和树的复杂度：\n\n基尼杂质是用来衡量数据集纯度的指标，它越小表示数据集的纯度越高。\n在剪枝过程中，我们希望保留那些对模型性能有积极影响的叶节点，同时减少模型的复杂度。\n当我们去掉一个叶节点时，基尼杂质会增加，因为我们丧失了这个叶节点的纯度。\n\n成本复杂度：\n\nccp_alpha 是一个成本复杂度参数，它在剪枝过程中平衡了模型的拟合程度和复杂度。\n当我们增大 ccp_alpha 时，模型更倾向于剪掉更多的叶节点，从而减少模型的复杂度。\n这样做的目的是防止过拟合，提高模型的泛化能力。\nYoutube video\nHow to Prune Regression Trees"
  },
  {
    "objectID": "wk6_classification1.html#random-forest",
    "href": "wk6_classification1.html#random-forest",
    "title": "6  Week7 - Classification-I",
    "section": "6.2 Random Forest",
    "text": "6.2 Random Forest"
  },
  {
    "objectID": "wk7_classification2.html#abstract",
    "href": "wk7_classification2.html#abstract",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.1 Abstract",
    "text": "7.1 Abstract\nHere is the overview of remote sensing techniques for classifying and assessing the accuracy of land cover data. Here are the key points:\n\n\n\n\n\n\nTip\n\n\n\n\nLandcover Classification: It discusses the use of pre-classified data sources like GlobeLand30, ESA’s CCI, Dynamic World, MODIS, and Google building data for landcover classification.\nDynamic World: The page details the process of training, pre-processing, normalization, and classification using CNNs, with a focus on Dynamic World’s semi-supervised approach and regional division for sample stratification.\nSub Pixel Analysis: It explains the concept of sub pixel classification, spectral mixture analysis, and linear spectral unmixing, including mathematical formulas for calculating the proportion of landcover per pixel.\nAccuracy Assessment: The page outlines various accuracy assessment methods in remote sensing, such as producer’s accuracy, user’s accuracy, overall accuracy, and the Kappa coefficient, along with their definitions and significance.\n\n\n\nBuilding upon the foundation laid in Week 6, where I explored the classification process and tackled challenges like overfitting in decision trees, this week delves deeper into the evaluation of classification models, specifically focusing on Accuracy Assessment and the Confusion Matrix.\nIn the context of remote sensing and geospatial analysis, accuracy assessment is critical to understanding how well a classification model performs. A classification model assigns labels to pixels or objects in an image based on the training data provided. However, to gauge the model’s effectiveness, we need to compare the predicted labels with the actual ground truth labels, which is where the confusion matrix comes into play."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "López-Puigdollers, Dan, Gonzalo Mateo-García, and Luis Gómez-Chova.\n2021. “Benchmarking Deep Learning Models for Cloud Detection in\nLandsat-8 and Sentinel-2 Images.” Remote Sensing 13 (5):\n992. https://doi.org/10.3390/rs13050992."
  },
  {
    "objectID": "wk2_xaringen.html#xaringen",
    "href": "wk2_xaringen.html#xaringen",
    "title": "2  Week 2 - Xaringen",
    "section": "2.1 Xaringen",
    "text": "2.1 Xaringen"
  },
  {
    "objectID": "wk3_RS_data.html#summary-of-common-remote-sensing-indices",
    "href": "wk3_RS_data.html#summary-of-common-remote-sensing-indices",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.2 Summary of Common Remote Sensing Indices",
    "text": "3.2 Summary of Common Remote Sensing Indices\nThe various techniques, including atmosphereic correction and imagery merging, used in remote sensing to enhance and process imagery data share common principles. While they serve different purposes1, they share some common principles:\n\nData Transformation: Most of these techniques involve transforming the raw data to enhance its usability and interpretability. This transformation can be radiometric, spatial, or spectral.\nEnhancement of Information: These methods aim to enhance the quality of the data to make it more useful for analysis. This could involve reducing noise, highlighting certain features, or combining data from different sources.\nMathematical and Statistical Methods: Many of these techniques rely on mathematical and statistical methods to process the data. This includes algorithms for filtering, statistical analysis for PCA, and mathematical models for atmospheric correction.\nImprovement of Visualization: Techniques such as enhancement and filtering are often used to improve the visualization of imagery, making it easier to interpret and analyze.\n\n\n\n\n\n\nRemote sensing process and ‘Remote Sensing for Earth Observation’ book chapters(Klaus Temfli et al., 2009)2\nIn remote sensing analysis, various indices are commonly used to extract information about land cover and land use. Here is a summary of some common remote sensing indices:\n\n\n\nSummary of Common Remote Sensing Indices\n\n\nIndex\nDescription\nFormula\n\n\n\n\nNormalized Difference Vegetation Index (NDVI)\nUsed to assess vegetation health and density by comparing the difference between near-infrared (NIR) and red band reflectance.\n$$NDVI= \\frac{NIR-Red}{NIR+Red}$$\n\n\nNormalized Difference Water Index (NDWI)\nUtilized to identify water bodies by comparing the difference between green and near-infrared band reflectance.\n$$NDWI = \\frac{(Green - NIR)}{(Green + NIR)}$$\n\n\nNormalized Difference Built-up Index (NDBI)\nEmployed to detect built-up areas by comparing the difference between near-infrared and shortwave infrared band reflectance.\n$$NDBI = \\frac{(SWIR - NIR)}{(SWIR + NIR)}$$\n\n\nSoil Adjusted Vegetation Index (SAVI)\nSimilar to NDVI but adjusts for soil background effects, particularly in areas with sparse vegetation cover.\n$$SAVI = \\frac{((NIR - Red) \\times (1 + L))}{(NIR + Red + L)}$$ where ( L ) is the soil adjustment factor.\n\n\nEnhanced Vegetation Index (EVI)\nAn enhanced version of NDVI that incorporates blue and red-edge bands, providing better sensitivity to high-density vegetation areas.\n$$EVI = G \\times \\frac{(NIR - Red)}{(NIR + C_1 \\times Red - C_2 \\times Blue + L)}$$ where G is the gain factor, C_1 and C_2 are coefficients, and L is the canopy background adjustment.\n\n\nSoil Moisture Index (SMI)\nUsed to estimate soil moisture content and soil wetness conditions in agricultural and hydrological studies.\n$$SMI = \\frac{(NIR + SWIR)}{2}$$\n\n\nLand Surface Temperature and Vegetation Index (LSTVI)\nCombines information from land surface temperature (LST) and vegetation indices to analyze urban heat island effects and land use changes.\n$$LSTVI = \\frac{(Tb - NDVI)}{(Tb + NDVI)}$$ where ( Tb ) is the land surface temperature.\n\n\n\n\n\n\n\nThese indices play crucial roles in various applications such as land cover classification, vegetation monitoring, and environmental assessments."
  },
  {
    "objectID": "wk3_RS_data.html#reflection",
    "href": "wk3_RS_data.html#reflection",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.4 Reflection",
    "text": "3.4 Reflection\nDuring my undergraduate dissertation project, I encountered challenges in processing remote sensing data due to significant smoke obscuring vegetation on the land surface. It was my first experience using Python to convert remote sensing images into multidimensional numerical matrices, where each dimension corresponded to different spectral bands of the original remote sensing data. Interestingly, the principle behind performing band calculations in Python was essentially the same as that of commonly used remote sensing index formulas. I realized that applying index calculation formulas in Python involved performing arithmetic operations such as addition, subtraction, multiplication, and division on these multidimensional numerical matrices.\nSo this is an example of the band processing I did in my undergraduate program to make the forest fire point more prominent in the image:\n\n\n\nComparison before and after band optimization\n\n\nThis realization underscores the significance of hands-on practice and theoretical knowledge integration in effectively utilizing remote sensing techniques for various environmental analyses and research endeavors."
  },
  {
    "objectID": "wk5_GEE.html#in-the-practical-exercises-section-i-learned",
    "href": "wk5_GEE.html#in-the-practical-exercises-section-i-learned",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.3 In the practical exercises section, I learned",
    "text": "5.3 In the practical exercises section, I learned\nAdvanced pixel-level image transformations: How to perform Principal Component Analysis (PCA) and Tasseled Cap transformations in GEE, which are methods for dimensionality reduction and feature extraction used for image classification and change detection in remote sensing imagery.\nGEE applications and data catalog: GEE can also create interactive visualization applications to showcase interesting and useful remote sensing analysis cases. GEE also provides a vast data catalog, including high-resolution satellite imagery, air pollution data, administrative boundaries, and other datasets."
  },
  {
    "objectID": "wk5_GEE.html#section",
    "href": "wk5_GEE.html#section",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.3 ",
    "text": "5.3"
  },
  {
    "objectID": "wk5_GEE.html#reflection",
    "href": "wk5_GEE.html#reflection",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.4 Reflection",
    "text": "5.4 Reflection\nReflecting on this week’s learning, I delved into the CASA0025: Building Spatial Applications with Big Data course, which shares similarities with our remote sensing syllabus!!! The collaboration between the two courses’ instructors is a testament to the interdisciplinary nature of these fields. It’s enlightening to see how knowledge from both domains can be integrated and enhance our understanding.\nOne aspect of GEE that stands out to me as particularly user-friendly, especially for engineers and scholars, is the accessibility of remote sensing data. Unlike proprietary data, GEE offers a wealth of openly available geospatial information. All that’s required is to pinpoint the specific time and location of interest and retrieve the corresponding API. This approach to data sharing is not only equitable but also fosters a spirit of community and collaboration within the field."
  },
  {
    "objectID": "wk4_policy.html#problem-context-background",
    "href": "wk4_policy.html#problem-context-background",
    "title": "4  Week4 - Policy Applications",
    "section": "4.1 Problem: Context & Background",
    "text": "4.1 Problem: Context & Background\nBeijing recently faced an unprecedented natural event: the heaviest rainfall in 140 years occurred between July 29 and August 1, 2023. This extreme weather event led to widespread flooding, impacting the city’s infrastructure, residents, and environment. Several factors contributed to the severity of the floods, including climate conditions, landform characteristics, and the effects of rapid urbanization.(Beijing Municipal Emergency Committee, 2022)1"
  },
  {
    "objectID": "wk4_policy.html#policy-case-study",
    "href": "wk4_policy.html#policy-case-study",
    "title": "4  Week4 - Policy Applications",
    "section": "4.2 Policy & Case Study",
    "text": "4.2 Policy & Case Study\n\n4.2.1 UN Sustainable Development Goals\nIn response to such climate-related challenges, it is crucial to align efforts with the United Nations’ Sustainable Development Goals (SDGs)2. Key objectives include prioritizing wastewater treatment and sustainable urbanization. By integrating these goals into urban planning and development, cities like Beijing can enhance their resilience to climate disasters. Additionally, following the Sendai Framework for Disaster Risk Reduction provides a comprehensive approach to disaster preparedness and mitigation.\n\n\n4.2.2 Beijing Urban Resilience Strategy\nBeijing’s urban resilience strategy aims to address vulnerabilities exacerbated by urbanization(Yuan,H., 2023)3. Key components include:\n\nUpgrading Emergency Disaster Prevention Technology: Investing in advanced monitoring systems, early warning mechanisms, and real-time data analytics can significantly enhance disaster preparedness and response.\nImproving Emergency Rescue Capability: Strengthening emergency services, training first responders, and establishing efficient evacuation protocols are essential for minimizing casualties during extreme events.\nFostering Cross-Sectoral Coordination and Collaboration: Effective disaster management requires collaboration among government agencies, private sectors, NGOs, and local communities. Coordinated efforts can enhance overall resilience and facilitate timely responses.\n\nBy implementing these strategies, Beijing can better withstand future climate-related challenges and protect its residents and infrastructure.\n\n\n\nConnect artificial or natural ponds and lakes with nearby rivers and create above-ground and groundwater systems for drainage and collecting excess water\n\n\n\n\n4.2.3 Beijing Sponge City Strategy\n“Sponge City” refers to an urban design approach that uses green infrastructure to absorb, store, and purify rainwater, thereby reducing flood risks and improving urban environmental quality(Xu, Y. , 2020)4. This design includes elements such as parks, permeable pavements, rain gardens, infiltration and storage wells, urban gardens, and green walls.\n This standard aims to scientifically and rationally compile sponge city planning, ensuring effective roles in water safety, resources, environment, ecology, and culture.\nOutlines requirements for compiling sponge city plans within Beijing’s administrative region, covering general urban planning, district planning, detailed planning, and town planning. Also includes assessment of sponge city planning implementation. These features not only facilitate the natural flow and storage of rainwater but also enhance the urban microclimate, increase biodiversity, and provide more recreational spaces for residents."
  },
  {
    "objectID": "wk4_policy.html#reference",
    "href": "wk4_policy.html#reference",
    "title": "4  Week4 - Policy Applications",
    "section": "4.4 Reference",
    "text": "4.4 Reference"
  },
  {
    "objectID": "wk6_classification1.html",
    "href": "wk6_classification1.html",
    "title": "6  Week7 - Classification-I",
    "section": "",
    "text": "6.1 Overfitting\n$$Tree score = SSR + tree penalty (alpha) * T (number of leaves)$$\nFor the Tree score formula, why does removing more leaves result in a larger alpha?\nDuring the pruning process of decision trees, we use a parameter called ccp_alpha (cost complexity parameter) to control the extent of pruning. A higher value of this parameter indicates a greater tendency to remove more leaf nodes, thereby simplifying the tree structure.\nLet’s understand why removing more leaf nodes leads to a larger ccp_alpha:\n6.1.1 Gini Impurity and Tree Complexity:\nGini impurity is a metric used to measure the purity of a dataset, with smaller values indicating higher purity.\nDuring pruning, we aim to retain leaf nodes that positively contribute to the model’s performance while reducing the complexity of the model.\nWhen we remove a leaf node, the Gini impurity increases because we lose the purity associated with that leaf node.\n6.1.2 Cost Complexity:\nccp_alpha is a cost complexity parameter that balances the fit and complexity of the model during pruning.\nBy increasing ccp_alpha, the model tends to remove more leaf nodes, thereby reducing the complexity of the model.\nThe purpose of this is to prevent overfitting and improve the generalization ability of the model."
  },
  {
    "objectID": "wk6_classification1.html#overfitting",
    "href": "wk6_classification1.html#overfitting",
    "title": "6  Week6 - Classification-I",
    "section": "6.3 Overfitting",
    "text": "6.3 Overfitting\n\\[Tree score = SSR + TreePenalty_{alpha}* T_{Mumber Of Leaves}\\]\nFor the Tree score formula, why does removing more leaves result in a larger alpha?\nDuring the pruning process of decision trees, we use a parameter called ccp_alpha (cost complexity parameter) to control the extent of pruning. A higher value of this parameter indicates a greater tendency to remove more leaf nodes, thereby simplifying the tree structure.\nLet’s understand why removing more leaf nodes leads to a larger ccp_alpha:\n\n6.3.1 Gini Impurity and Tree Complexity:\nGini impurity is a metric used to measure the purity of a dataset, with smaller values indicating higher purity.\nDuring pruning, we aim to retain leaf nodes that positively contribute to the model’s performance while reducing the complexity of the model.\nWhen we remove a leaf node, the Gini impurity increases because we lose the purity associated with that leaf node.\n\n\n\nRepresentation of a decision tree before and after being processed by a pruning algorithm, where the decision nodes (light blue) classify samples into 2 classes (green and red). The red dividing lines represent the pruning step.\n\n\n\n\n6.3.2 Cost Complexity:\nccp_alpha is a cost complexity parameter that balances the fit and complexity of the model during pruning.\nBy increasing ccp_alpha, the model tends to remove more leaf nodes, thereby reducing the complexity of the model.\nThe purpose of this is to prevent overfitting and improve the generalization ability of the model."
  },
  {
    "objectID": "wk9_SAR_data.html#section",
    "href": "wk9_SAR_data.html#section",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.3 ",
    "text": "8.3"
  },
  {
    "objectID": "wk9_SAR_data.html#sar-data-application",
    "href": "wk9_SAR_data.html#sar-data-application",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.4 SAR data application",
    "text": "8.4 SAR data application\nAnalyze the change between two images (e.g. ratio or logarithmic ratio)\nOpen Access Damage Detection Using Sentinel-1 Imagery\nBlast Damage Assessment\nSee the difference over time in the following ways:\nPixel-Wise T-Test"
  },
  {
    "objectID": "wk4_policy.html#beijing-sponge-city-strategy",
    "href": "wk4_policy.html#beijing-sponge-city-strategy",
    "title": "4  Week4 - Policy Applications",
    "section": "4.3 Beijing Sponge City Strategy",
    "text": "4.3 Beijing Sponge City Strategy\n\nThis standard aims to scientifically and rationally compile sponge city planning, ensuring effective roles in water safety, resources, environment, ecology, and culture.\n\nIssuing Bodies: Compiled by the Beijing Urban Planning and Design Research Institute and the China Urban Planning and Design Research Institute.\nApproval Departments: Approved by the Beijing Municipal Planning and Natural Resources Committee and the Beijing Municipal Market Supervision Administration.\nImplementation Date: Came into effect on January 1, 2021.\nContent Summary: Outlines requirements for compiling sponge city plans within Beijing’s administrative region, covering general urban planning, district planning, detailed planning, and town planning. Also includes assessment of sponge city planning implementation."
  },
  {
    "objectID": "wk4_policy.html#reflection",
    "href": "wk4_policy.html#reflection",
    "title": "4  Week4 - Policy Applications",
    "section": "4.4 Reflection",
    "text": "4.4 Reflection\nIn this course, I gained valuable insights from examining a city’s policies related to natural disaster protection. Among the new concepts I encountered, the most memorable one was the idea of a “sponge city.” This term vividly illustrates Beijing’s approach to mitigating flood hazards by implementing innovative measures.\nThe concept of a sponge city emphasizes the need to absorb and manage excess water during heavy rainfall, thereby reducing the risk of flooding. Beijing’s strategy involves creating permeable surfaces, enhancing green spaces, and improving drainage systems. Witnessing how a city like Beijing tackles flood challenges left a lasting impression on me.\nOne significant challenge in implementing urban policies is striking a balance between macro-level planning and local adjustments. How can a city retain its existing infrastructure while making targeted changes to enhance resilience? This delicate balance requires thoughtful consideration and cross-disciplinary collaboration.\nOverall, this course highlighted the complexities of urban resilience and the importance of forward-thinking policies in safeguarding cities against natural disasters. As I continue my studies, I’ll keep the lessons from this class in mind, especially the innovative approaches like sponge cities that can shape our urban future."
  },
  {
    "objectID": "wk5_GEE.html#application",
    "href": "wk5_GEE.html#application",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\n\n5.2.1 Example\nGoogle Earth Engine (GEE) is a powerful geospatial processing service that leverages cloud servers to store and analyze vast amounts of remote sensing imagery and geospatial data(Noel Gorelick et al., 2017)1. Its main features include: Rapid Processing ( Capable of quickly and massively monitoring and simulating changes on the Earth’s surface), Diverse Data Types (Supports both raster and vector data, corresponding to images and features respectively), Advanced Functions (Offers advanced data processing capabilities such as regression, joins, and machine learning).\nHere’s a summary of the Google Earth Engine (GEE) applications(Rafael Louzeiro, 2021)2:\n\nRemote Sensing: GEE processes remote sensing data, useful for environmental and climate analysis.\nHistorical Imagery: It stores historical satellite imagery for geospatial analyses, like forest and water coverage.\nData Access: Users can access a vast database of pre-processed datasets and imagery for their own analysis1.\nReal-World Applications: GEE is applied in various fields, including vegetation analysis, land cover studies, and natural disaster management.\n\nThis picture provides an overview of how GEE can be utilized in environmental science, highlighting its capabilities and benefits.(Meisam Amani et al., 2020)3\n\n\n\nGEE applications (Meisam Amani 2020)\n\n\n\n\n5.2.2 Earth Engine Explorer (EE Explorer)\nEE Explorer is a lightweight geospatial image data viewer that provides access to a wide range of global and regional datasets from the Earth Engine Data Catalog. It allows users to quickly view, zoom, and pan data anywhere on Earth, adjust visualization settings, and layer data to inspect changes over time( Google Earth Engine, 2024)4.\nComponents:\n\nData Catalog: This is where users discover and import datasets. It lists various data types and multi-day mosaics, with descriptions and direct links to datasets.\nWorkspace: This is where users manage and visualize datasets. It includes a map and a list of data layers. Users can add datasets, adjust visualization settings, and view multiple data layers simultaneously.\n\nFeatures:\n\nVisualization Settings: Users can adjust visualization parameters such as range, gamma, and opacity. Data can be viewed in single-band grayscale, single-band pseudo-color, and three-band RGB.\nChange Over Time: Users can visualize changes over time by adding the same dataset as two separate layers and setting them to show different time slices.\nGoogle Earth Engine: For more advanced features like classifying land cover, downloading datasets, and building data analysis algorithms, users can sign up for Google Earth Engine.\n\n\n\n\nWorking place of GEE\n\n\nGetting Started with EE Explorer: There are several tutorials available to help us get started. These tutorials cover everything from basic navigation to advanced data visualization techniques. We can explore these resources through the following link:\n\nBeginner’s Cookbook with EE Explorer\n\n\n\n\nGet start with GEE\n\n\nBy following these tutorials, users can become familiar with the interface, learn how to access and visualize various datasets, and understand how to utilize EE Explorer’s features for their specific needs."
  },
  {
    "objectID": "wk5_GEE.html#references",
    "href": "wk5_GEE.html#references",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.5 References",
    "text": "5.5 References"
  },
  {
    "objectID": "wk8_SAR_data.html#abstract",
    "href": "wk8_SAR_data.html#abstract",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.1 Abstract",
    "text": "8.1 Abstract\nThis class introduces the basic principle of synthetic aperture radar (SAR), two sensor types, SAR data background, SAR data value, SAR polarization, SAR background, differential interferometric synthetic aperture radar (DlnSAR), SAR data processing, SAR data fusion, SAR image fusion and other concepts. This paper also introduces how to use SAR data for change detection, including statistical test, threshold screening and ROC curve, image fusion and so on. Finally, a change detection algorithm based on SAR data is proposed, and an example is given."
  },
  {
    "objectID": "wk8_SAR_data.html#difference-of-other-sar",
    "href": "wk8_SAR_data.html#difference-of-other-sar",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.1 Difference of other SAR",
    "text": "8.1 Difference of other SAR\n\n\n\n\n\n\n\n\n\nSAR\nInSAR\nDInSAR\nPSInSAR\n\n\n\n\nSynthetic Aperture Radar\nInterferometric Synthetic Aperture Radar\nDifferential Interferometric Synthetic Aperture Radar\nPersistent Scatterer Interferometric SAR\n\n\n\nA radar system capable of producing high-resolution images.\nUses a “virtual” antenna length to combine echo signals received from different positions, resulting in higher-resolution radar imaging.\nUsed for surface observations such as land use, topography, and forest cover.\n\n\nAnalyzes surface deformation by exploiting the phase difference between two remote sensing images.\nCalculates the deformation at each pixel on the ground surface between two observations.\nReveals elevation and deformation information.\n\n\nBuilds upon InSAR by using phase differences from multiple remote sensing images to improve deformation measurement accuracy.\nSensitive to deformation, suitable for monitoring ground surface changes due to earthquakes, mining, landslides, etc.\nUtilizes two SAR images and external topographic data to measure subtle surface deformations.\n\n\nModels and analyzes time series data from multiple SAR images to enhance deformation inversion accuracy.\nReveals spatial distribution of surface deformations, widely used for monitoring urban subsidence and infrastructure changes."
  },
  {
    "objectID": "wk8_SAR_data.html#section",
    "href": "wk8_SAR_data.html#section",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.3 ",
    "text": "8.3"
  },
  {
    "objectID": "wk8_SAR_data.html#sar-data-application",
    "href": "wk8_SAR_data.html#sar-data-application",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.2 SAR data application",
    "text": "8.2 SAR data application\nAnalyze the change between two images (e.g. ratio or logarithmic ratio)\nOpen Access Damage Detection Using Sentinel-1 Imagery\nBlast Damage Assessment\nSee the difference over time in the following ways:\nPixel-Wise T-Test\n\nUrban Infrastructure Monitoring: SAR data, particularly PSInSAR, is widely used for detecting subsidence and other structural changes in urban environments. This is crucial for monitoring infrastructure health and urban development (Ferretti et al., 20011).\nDisaster Management: DInSAR has proven highly effective in disaster scenarios, such as earthquakes and landslides. By comparing radar images from different times, DInSAR can detect ground displacement and deformation with millimeter accuracy (Hanssen, 20012).\nLand Cover Change Detection: SAR data is used for deforestation monitoring and agricultural applications. For example, by analyzing SAR time series, changes in forest cover can be detected, even under cloud cover conditions."
  },
  {
    "objectID": "wk6_classification1.html#reflection",
    "href": "wk6_classification1.html#reflection",
    "title": "6  Week6 - Classification-I",
    "section": "6.4 Reflection",
    "text": "6.4 Reflection\nThis week’s focus on classification techniques using Google Earth Engine (GEE) highlighted the critical challenge of overfitting in remote sensing models. Overfitting leads to models that perform well on training data but struggle with new data. The introduction of cost complexity pruning, particularly the role of ccp_alpha, emphasized the need to balance model complexity and generalization.\nWorking with Random Forest classifiers in GEE provided practical insights into managing this balance. The process of selecting training data, evaluating models, and understanding the impact of pruning deepened my understanding of how to create robust models that can adapt to diverse environmental conditions.\nAs I progress, I’m eager to apply these concepts to more complex datasets, refining strategies to improve model performance while avoiding overfitting. Balancing detailed data capture with model robustness remains a key focus in my ongoing work."
  },
  {
    "objectID": "wk7_classification2.html#reflection",
    "href": "wk7_classification2.html#reflection",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.6 Reflection",
    "text": "7.6 Reflection\nContinuing from the challenges discussed in Week 6, this week’s focus on accuracy and the confusion matrix highlights the importance of rigorous model evaluation in remote sensing. Ensuring that a classification model is not only accurate but also balanced and reliable across different classes is essential for effective geospatial analysis. This week has deepened my understanding of the critical role that accuracy assessment plays in model validation, and I am eager to apply these insights to improve the robustness of my classification models in future projects.\nThis reminds me of my undergraduate dissertation project, where I evaluated the accuracy of fire detection models using confusion matrices.\n\n\n\nConfusion Matrix\n\n\nFirstly, I prepared the data by creating binary classification labels using both active detection thresholding and visual inspection methods as ground truth data. Next, I trained the model using a U-net neural network model on remote sensing images, applying both cross-entropy and focal loss functions. Finally, I made predictions and evaluations by predicting fire pixel locations in new images and assessing the differences between expected and original images using confusion matrix evaluation metrics to determine prediction accuracy. Evaluating accuracy was a crucial practical experience for me in using convolutional networks to predict fire spots in remote sensing images."
  },
  {
    "objectID": "intro.html#application",
    "href": "intro.html#application",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.3 Application",
    "text": "1.3 Application\n\n1.3.1 Remote Sensing Bands and Environmental Applications\nThis section explores how different spectral bands used in remote sensing contribute to monitoring and managing environmental phenomena such as forest vegetation, water bodies, and soil classification. Each application highlights specific bands that enhance detection accuracy and provide critical insights for environmental studies and management.\n\n\n\nEnvironmental Application\nExample\n\n\n\n\nForest Vegetation\n\n\nSensitive Bands:\nRed, Near-Infrared (NIR)\nDeforestation Monitoring\n\nClearcuts in radar images (white boxes). The radar backscatter differences in the SAR image allow to distinguish between forest and non-forest areas and make it possible to map and measure the extent of deforestation.\n\n\nFor change detection analysis, a stack of radar images is needed. Figure 4 shows an example for ongoing deforestation (white polygons) in the Guaviare Department, Colombia, between January and May 2020.\n\n\n\nWater Bodies\n\nSensitive Bands: Blue, Green\nDevendra Singh et al. (2023)2 have leveraged geospatial modeling systems and advanced spatial and spectral resolution sensors to monitor various factors related to water quality at an affordable cost and with higher accuracy, including turbidity, chlorophyll-a, suspended residues, and algal blooms in different water bodies. This chapter provides a detailed review of the role of Geographic Information Systems (GIS) and remote sensing in the monitoring, management, and remediation of water quality.\n\n\nSoil Classification\n\nSensitive Bands: Red, Shortwave Infrared (SWIR)\nHyperspectral remote sensing (HRS) (Huan Yu et al, 2020)3 is used for a detailed analysis of soil spectral properties, with extensive research over the past 40 years. HRS helps identify various soil properties such as minerals, nutrients, organic carbon, moisture, salinity, and texture."
  },
  {
    "objectID": "intro.html#summary",
    "href": "intro.html#summary",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nBefore starting this course, I need to first understand the purpose of this course, which is to introduce the basic concepts and applications of remote sensing, and through this course of RS, the content we will learn includes:\n\n\n\n\n\n\nSummary of the first Class\n\n\n\n\nTele-sensing Cities and Environments: This is a course that introduces the basic principles and applications of tele-sensing, with a focus on urban and environmental issues.\nTypes and principles of remote sensing: Remote sensing is divided into active and passive, depending on whether it has its own energy transmitter. Remote sensing data is affected by the interaction of electromagnetic waves with the atmosphere and the surface, and needs to be corrected and processed.\nFormat and resolution of remote sensing data: Remote sensing data is usually raster data and has different storage formats and ways of organizing. The quality and application of remote sensing data are constrained by spatial, spectral, temporal and radiative resolution.\n\n\n\nIt easily reminds me of the basic concepts of RS that I learned by myself during my undergraduate study. I believe that this course can not only help me review the past knowledge, but also fully supplement other knowledge."
  },
  {
    "objectID": "intro.html#application2",
    "href": "intro.html#application2",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.4 Application2",
    "text": "1.4 Application2\n\n1.4.1 Remote Sensing Bands and Environmental Applications\n\n\n\nEnvironmental Application\nSensitive Bands\nExample\n\n\n\n\nForest Vegetation\nRed, Near-Infrared (NIR)\nDeforestation Monitoring\n\nClearcuts in radar images (white boxes). The radar backscatter differences in the SAR image allow to distinguish between forest and non-forest areas and make it possible to map and measure the extent of deforestation.\n\n\nFor change detection analysis, a stack of radar images is needed. Figure 4 shows an example for ongoing deforestation (white polygons) in the Guaviare Department, Colombia, between January and May 2020.\n\n\n\nWater Bodies\nBlue, Green\nLow reflectance in water but higher in suspended sediments and algae. Used for water quality.\n\n\nSoil Classification\nRed, Shortwave Infrared (SWIR)\nDifferentiates soil types based on moisture content and mineral composition.\n\n\nFire Detection\nMid-Infrared (MIR), Thermal IR (TIR)\nDetects active fires and monitors post-fire surface temperature changes.\n\n\n\n\n\n1.4.2 Summary\n\n1.4.2.1 Forest Vegetation\n\nSensitive Bands: Red and Near-Infrared (NIR)\nBand Combination: Red/NIR\nDescription: High reflectance in NIR and low in Red for healthy vegetation. Used in NDVI calculation.\n\n\n\n1.4.2.2 Water Bodies\n\nSensitive Bands: Blue and Green\nBand Combination: Blue/Green\nDescription: Low reflectance in water but higher in suspended sediments and algae. Used for water quality monitoring.\n\n\n\n1.4.2.3 Soil Classification\n\nSensitive Bands: Red and Shortwave Infrared (SWIR)\nBand Combination: Red/SWIR\nDescription: Differentiates soil types based on moisture content and mineral composition.\n\n\n\n1.4.2.4 Fire Detection\n\nSensitive Bands: Mid-Infrared (MIR) and Thermal Infrared (TIR)\nBand Combination: MIR/TIR\nDescription: Detects active fires and monitors post-fire surface temperature changes."
  },
  {
    "objectID": "wk3_RS_data.html#application",
    "href": "wk3_RS_data.html#application",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.3 Application",
    "text": "3.3 Application\nIn practical applications, these indices are not used in isolation but are often combined to enhance the interpretation and extraction of information from imagery data. Combining multiple indices in remote sensing analysis can provide more comprehensive insights and improve the accuracy of the results.\nKim, S.W et al.(2021)3, discusses various indices used to estimate and analyze urban heat island (UHI) intensity and magnitude. It highlights the use of NDVI for extracting land surface temperature data from satellite images, NDBI for analyzing the built environment, EVI for assessing vegetation coverage, and the Biophysical Composition Index (BCI) for describing urban biophysical characteristics. These indices help researchers to more accurately estimate and analyze UHI intensity and magnitude.\nSchultz, M. et al.(2016)4 evaluates the performance of eight vegetation indices (VI) from Landsat time series (LTS) for monitoring deforestation across tropical regions. It uses a robust reference database to assess spatial accuracy, sensitivity to observation frequency, and the performance of combined VI. The study mentions the use of the Normalized Difference Fraction Index (NDFI) sensitive to canopy cover, moisture-related VIs (Normalized Difference Moisture Index (NDMI) and Tasseled Cap Wetness (TCw)) that performed spatially better than greenness-related VIs (Normalized Difference Vegetation Index (NDVI) and Tasseled Cap Greenness (TCg)). The spatial accuracy improved, and the overestimation of changes decreased when VIs were fused at the feature level.\n\n\n\nUHI Analysis Image\n\n\nThese examples illustrate how combining multiple indices can enhance the analysis and provide more accurate and detailed information for various remote sensing applications."
  },
  {
    "objectID": "wk3_RS_data.html#footnotes",
    "href": "wk3_RS_data.html#footnotes",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "",
    "text": "Lillesand, T.M. & Kiefer, R.W., 1987. Remote sensing and image interpretation. 2nd ed1. New York: Wiley. https://archive.org/details/remotesensingima00lill/page/1/mode/1up↩︎\nTempfli, K, Huurneman, GC, Bakker, WH, Janssen, LLF, Feringa, WF, Gieske, ASM, Grabmaier, KA, Hecker, CA, Horn, JA, Kerle, N, van der Meer, FD, Parodi, GN, Pohl, C, Reeves, CV, van Ruitenbeek, FJA, Schetselaar, EM, Weir, MJC, Westinga, E & Woldai, T 2009, Principles of remote sensing: an introductory textbook. ITC Educational Textbook Series, vol. 2, International Institute for Geo-Information Science and Earth Observation, Enschede. http://www.itc.nl/library/papers_2009/general/PrinciplesRemoteSensing.pdf↩︎\nKim, S.W. and Brown, R., 2021. “Urban Heat Island (UHI) intensity and magnitude estimation: A systematic literature review.” Science of The Total Environment, 779, p.146389. DOI:10.1016/j.scitotenv.2021.146389↩︎\nSchultz, M., Verbesselt, J.G.P.W., Carter, S., Verbesselt, J., Avitabile, V., Quang, H.V., & Herold, M. (2016). ‘Performance of Landsat time series vegetation indices for monitoring deforestation’, International Journal of Applied Earth Observation and Geoinformation, vol. 52, pp. 318-327.↩︎"
  },
  {
    "objectID": "wk3_RS_data.html#summary",
    "href": "wk3_RS_data.html#summary",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.2 Summary",
    "text": "3.2 Summary\nThe various techniques, including atmospheric correction and imagery merging, used in remote sensing to enhance and process imagery data share common principles. While they serve different purposes1, they share some common principles:\n\nData Transformation: Most of these techniques involve transforming the raw data to enhance its usability and interpretability. This transformation can be radiometric, spatial, or spectral.\nEnhancement of Information: These methods aim to enhance the quality of the data to make it more useful for analysis. This could involve reducing noise, highlighting certain features, or combining data from different sources.\nMathematical and Statistical Methods: Many of these techniques rely on mathematical and statistical methods to process the data. This includes algorithms for filtering, statistical analysis for PCA, and mathematical models for atmospheric correction.\nImprovement of Visualization: Techniques such as enhancement and filtering are often used to improve the visualization of imagery, making it easier to interpret and analyze.\n\n\n\n\n\n\nRemote sensing process and ‘Remote Sensing for Earth Observation’ book chapters(Klaus Temfli et al., 2009)2\nIn remote sensing analysis, the Normalized Difference Vegetation Index (NDVI) is a widely used index for assessing vegetation health and density by comparing the difference between near-infrared (NIR) and red band reflectance. \\[NDVI= \\frac{NIR-Red}{NIR+Red}\\]\nBesides NDVI, various other indices are commonly used to extract information about land cover and land use, enhancing the overall information derived from remote sensing data. Here is a summary of some common remote sensing indices.\n\n\n\nSummary of Common Remote Sensing Indices\n\n\nIndex\nDescription\nFormula\n\n\n\n\nNormalized Difference Water Index (NDWI)\nUtilized to identify water bodies by comparing the difference between green and near-infrared band reflectance.\n$$NDWI = \\frac{(Green - NIR)}{(Green + NIR)}$$\n\n\nNormalized Difference Built-up Index (NDBI)\nEmployed to detect built-up areas by comparing the difference between near-infrared and shortwave infrared band reflectance.\n$$NDBI = \\frac{(SWIR - NIR)}{(SWIR + NIR)}$$\n\n\nSoil Adjusted Vegetation Index (SAVI)\nSimilar to NDVI but adjusts for soil background effects, particularly in areas with sparse vegetation cover.\n$$SAVI = \\frac{((NIR - Red) \\times (1 + L))}{(NIR + Red + L)}$$ where ( L ) is the soil adjustment factor.\n\n\nEnhanced Vegetation Index (EVI)\nAn enhanced version of NDVI that incorporates blue and red-edge bands, providing better sensitivity to high-density vegetation areas.\n$$EVI = G \\times \\frac{(NIR - Red)}{(NIR + C_1 \\times Red - C_2 \\times Blue + L)}$$ where G is the gain factor, C_1 and C_2 are coefficients, and L is the canopy background adjustment.\n\n\nSoil Moisture Index (SMI)\nUsed to estimate soil moisture content and soil wetness conditions in agricultural and hydrological studies.\n$$SMI = \\frac{(NIR + SWIR)}{2}$$\n\n\nLand Surface Temperature and Vegetation Index (LSTVI)\nCombines information from land surface temperature (LST) and vegetation indices to analyze urban heat island effects and land use changes.\n$$LSTVI = \\frac{(Tb - NDVI)}{(Tb + NDVI)}$$ where ( Tb ) is the land surface temperature.\n\n\n\n\n\n\n\nThese indices play crucial roles in various applications such as land cover classification, vegetation monitoring, and environmental assessments."
  },
  {
    "objectID": "wk4_policy.html#footnotes",
    "href": "wk4_policy.html#footnotes",
    "title": "4  Week4 - Policy Applications",
    "section": "",
    "text": "United Nations Department of Economic and Social Affairs (UNDESA). (n.d.)Milestones for Inclusive Social Development. Retrieved from [https://social.desa.un.org/sdn/transforming-our-world-the-2030-agenda-for-sustainable-development].↩︎\nUnited Nations Department of Economic and Social Affairs (UNDESA). (n.d.)Milestones for Inclusive Social Development. Retrieved from [https://social.desa.un.org/sdn/transforming-our-world-the-2030-agenda-for-sustainable-development].↩︎\nYuan, H.Proposals for China’s Vision for 2035 (2023) Available at: https://www.mem.gov.cn/xw/ztzl/2020/xxgcwzqh/qwjd/zjjd/202012/t20201208_374880.shtml (Accessed: 10 March 2024).↩︎\nXu, Y. (2020). ‘Sponge Cities: A Sustainable Urban Design Approach’, Chapman Taylor. Available at: [https://www.chapmantaylor.com/insights/what-are-sponge-cities-and-why-are-they-the-future-of-urban-design] (Accessed: 31 August 2024).↩︎\nSustainable Development Goals (SDG) 11.2015. United NationsSDG 11 - Sustainable Cities and Communities↩︎\nSustainable Development Goals (SDG) 6.2015. United NationsSDG 6 - Clean Water and Sanitation↩︎"
  },
  {
    "objectID": "wk3_RS_data.html",
    "href": "wk3_RS_data.html",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "",
    "text": "4 Reference"
  },
  {
    "objectID": "wk4_policy.html#relationship-between-urban-resilience-strategy-sponge-city",
    "href": "wk4_policy.html#relationship-between-urban-resilience-strategy-sponge-city",
    "title": "4  Week4 - Policy Applications",
    "section": "4.3 Relationship between Urban Resilience Strategy & “Sponge City”",
    "text": "4.3 Relationship between Urban Resilience Strategy & “Sponge City”"
  },
  {
    "objectID": "wk4_policy.html#urban-resilience-strategy-v.s.-sponge-city",
    "href": "wk4_policy.html#urban-resilience-strategy-v.s.-sponge-city",
    "title": "4  Week4 - Policy Applications",
    "section": "4.3 Urban Resilience Strategy v.s. “Sponge City”",
    "text": "4.3 Urban Resilience Strategy v.s. “Sponge City”\nRelationship Between Sponge City and Urban Resilience:\nThe “Sponge City” concept is a nature-based approach designed to enhance urban resilience by improving water management and reducing the impact of extreme weather events. In essence, a “Sponge City” mimics natural hydrological processes by absorbing, storing, and purifying rainwater, thereby reducing urban flooding risks and replenishing groundwater. This strategy helps cities to adapt to climate change by making them more capable of withstanding and recovering from environmental stresses, particularly those related to water.\nSupporting SDGs:\n\nSDG 11 - Sustainable Cities and Communities: The “Sponge City” strategy directly supports SDG 11(United Nations, 2015)5 by promoting resilient infrastructure and sustainable urban development. It contributes to making cities inclusive, safe, resilient, and sustainable by reducing the vulnerability of urban areas to flooding, enhancing public spaces, and improving the quality of urban life.\nSDG 6 - Clean Water and Sanitation: The “Sponge City” approach also aligns with SDG 6(United Nations, 2015)6, which aims to ensure availability and sustainable management of water and sanitation for all. By improving urban water management, the “Sponge City” concept helps in the efficient use of water resources, reducing the risk of water scarcity, and ensuring cleaner water supply by filtering pollutants naturally.\n\n\n4.3.1 Reflection\nIn this course, I gained valuable insights from examining a city’s policies related to natural disaster protection. Among the new concepts I encountered, the most memorable one was the idea of a “sponge city.” This term vividly illustrates Beijing’s approach to mitigating flood hazards by implementing innovative measures.\nThe concept of a sponge city emphasizes the need to absorb and manage excess water during heavy rainfall, thereby reducing the risk of flooding. Beijing’s strategy involves creating permeable surfaces, enhancing green spaces, and improving drainage systems. Witnessing how a city like Beijing tackles flood challenges left a lasting impression on me.\nOne significant challenge in implementing urban policies is striking a balance between macro-level planning and local adjustments. How can a city retain its existing infrastructure while making targeted changes to enhance resilience? This delicate balance requires thoughtful consideration and cross-disciplinary collaboration.\nOverall, this course highlighted the complexities of urban resilience and the importance of forward-thinking policies in safeguarding cities against natural disasters. As I continue my studies, I’ll keep the lessons from this class in mind, especially the innovative approaches like sponge cities that can shape our urban future."
  },
  {
    "objectID": "wk5_GEE.html#summary-of-class5",
    "href": "wk5_GEE.html#summary-of-class5",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.1 Summary of class5",
    "text": "5.1 Summary of class5\nThe theoretical part of the course mainly introduced the basic concepts, data structures, operations, and application cases of Google Earth Engine (GEE).\n\n\n\n\n\n\nTip\n\n\n\nCharacteristics of GEE: GEE is a geospatial processing service that utilizes cloud servers to store and analyze massive amounts of remote sensing imagery and geographic data, enabling rapid and large-scale monitoring and simulation of changes on the Earth’s surface.\nData structure of GEE: Data in GEE is divided into two types: Images and Features, corresponding to raster and vector data, respectively. Images and Features can form Collections, representing stacks of multiple images or features. Data in GEE exists in the form of objects, each with its own properties and methods.\nOperations in GEE: GEE uses the JavaScript language for coding, which can be run in the browser. The code in GEE is divided into client-side and server-side, with the client-side code mainly used for interface control and interaction, and the server-side code mainly used for data processing and analysis. Data processing in GEE mainly relies on reducers, which can perform various statistical, analytical, and transformation operations on images or features. GEE also provides some advanced features such as regression, joining, and machine learning."
  },
  {
    "objectID": "wk5_GEE.html#footnotes",
    "href": "wk5_GEE.html#footnotes",
    "title": "5  Week5 - Google Earth Engine",
    "section": "",
    "text": "Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., & Moore, R. (2017). Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote Sensing of Environment. DOI:https://doi.org/10.1016/j.rse.2017.06.031↩︎\nRafael Louzeiro – GIS/Environmental Specialist, Google Earth Engine – How it can be used in Environmental Science1, Jun 3, 2021, Integrate Sustainability Pty Ltd, https://www.integratesustainability.com.au/2021/06/03/google-earth-engine-how-it-can-be-used-in-environmental-science/↩︎\nMeisam Amani, Senior Member, IEEE, Arsalan Ghorbanian , Seyed Ali Ahmadi , Mohammad Kakooei , Armin Moghimi , S. Mohammad Mirmazloumi, Student Member, IEEE, Sayyed Hamed Alizadeh Moghaddam , Sahel Mahdavi, Masoud Ghahremanloo, Saeid Parsian, Qiusheng Wu. 2020. Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review. IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 13,.↩︎\nGoogle Earth Engine (2024) ‘Introduction to Google Earth Engine’, Google Earth Engine. source: https://www.google.com/earth/outreach/learn/introduction-to-google-earth-engine (Assessed day: September 1st, 2024).↩︎"
  },
  {
    "objectID": "wk6_classification1.html#summary",
    "href": "wk6_classification1.html#summary",
    "title": "6  Week6 - Classification-I",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nIn this week, we explore the use of Google Earth Engine (GEE) for geospatial analysis. We start by loading administrative boundary data from the FAO GAUL global admin layers and proceed to work with Sentinel-2 surface reflectance data. The session covers cloud masking techniques, classification using supervised classifiers like Random Forest, and accuracy assessment.\n\n\n\n\n\n\nTip\n\n\n\nGEE Data Catalog: The class discusses how to search and load data from the Google Earth Engine (GEE) data catalog, specifically focusing on FAO GAUL global admin layers and Sentinel data.\nScript and Code Examples: It provides example code for loading and filtering data, applying cloud masks, and handling Sentinel data.\nClassification Process: The page explains the process of classification using supervised classifiers like CART, RandomForest, NaiveBayes, and SVM, including steps for training and validating the model.\nTraining Data: Instructions are given on how to select and use training data within the study area, including generating samples and merging polygons into a feature collection."
  },
  {
    "objectID": "wk6_classification1.html#classification-process",
    "href": "wk6_classification1.html#classification-process",
    "title": "6  Week6 - Classification-I",
    "section": "6.2 Classification Process",
    "text": "6.2 Classification Process\nClassification is a supervised learning technique used to categorize data points into predefined classes or categories based on input features. The goal is to learn a mapping from input variables to a set of discrete outcomes (labels) using a training dataset where the correct labels are known.\n\n6.2.1 Principles of Classification(Sokal, R. R. , 19741; Zonneveld, I. S. , 19942)\n\nTraining Phase: During this phase, the classifier learns from labeled training data by identifying patterns and relationships between the input features and their corresponding labels. Popular algorithms for classification include decision trees, random forests, support vector machines, and neural networks.\nPrediction Phase: After training, the classifier is used to predict the class of new, unseen data points based on the patterns it has learned.\nEvaluation: The classifier’s performance is evaluated using various metrics like accuracy, precision, recall, and the F1 score. Cross-validation is often employed to ensure the model generalizes well to new data.\n\n\n\n6.2.2 Applications of Classification\nClassification is widely used across various domains, including:\n\nMedical Diagnosis:\n\nClassification techniques, particularly deep learning, play a crucial role in medical diagnostics. According to Mihalj Bakator and Mihalj Bakator (2018)3, deep learning methods like Convolutional Neural Networks (CNNs) are extensively used in tasks such as diagnosis, classification, and prediction within medical image analysis. Although deep learning has shown significant potential, it is not yet capable of fully replacing doctors, highlighting the need for a collaborative approach between technology and human expertise.\n\n\n\nFlow diagram of the review process\n\n\n\nSpam Detection: Email filtering systems classify emails as “spam” or “not spam” using algorithms like Naive Bayes(A. Almomani et al., 20134; Bahgat et al., 20165).\n\n\n\n\nAn example of message structure for the purpose of feature"
  },
  {
    "objectID": "wk6_classification1.html#reference",
    "href": "wk6_classification1.html#reference",
    "title": "6  Week6 - Classification-I",
    "section": "6.5 Reference",
    "text": "6.5 Reference"
  },
  {
    "objectID": "wk6_classification1.html#footnotes",
    "href": "wk6_classification1.html#footnotes",
    "title": "6  Week6 - Classification-I",
    "section": "",
    "text": "Sokal, R. R. (1974). Classification: Purposes, Principles, Progress, Prospects: Clustering and other new techniques have changed classificatory principles and practice in many sciences. Science, 185(4157), 1115-1123.↩︎\nZonneveld, I. S. (1994). Basic principles of classification. In Ecosystem classification for environmental management (pp. 23-47). Dordrecht: Springer Netherlands.↩︎\nBakator, Mihalj, and Dragica Radosav. 2018. “Deep Learning and Medical Diagnosis: A Review of Literature” Multimodal Technologies and Interaction 2, no. 3: 47. https://doi.org/10.3390/mti2030047↩︎\nA. Almomani, B. B. Gupta, S. Atawneh, A. Meulenberg and E. Almomani, “A Survey of Phishing Email Filtering Techniques,” in IEEE Communications Surveys & Tutorials, vol. 15, no. 4, pp. 2070-2090, Fourth Quarter 2013, doi: 10.1109/SURV.2013.030713.00020.↩︎\nBahgat, E.M., Rady, S., Gad, W. (2016). An E-mail Filtering Approach Using Classification Techniques. In: Gaber, T., Hassanien, A., El-Bendary, N., Dey, N. (eds) The 1st International Conference on Advanced Intelligent System and Informatics (AISI2015), November 28-30, 2015, Beni Suef, Egypt. Advances in Intelligent Systems and Computing, vol 407. Springer, Cham. https://doi.org/10.1007/978-3-319-26690-9_29↩︎\nFriedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. (No Title).↩︎\nQuinlan, J. R. (2014). C4. 5: programs for machine learning. Elsevier.↩︎\nBreiman, L. (2017). Classification and regression trees. Routledge.↩︎\nQuinlan, J. R. (2014). C4. 5: programs for machine learning. Elsevier.↩︎\nBreiman, L. (2017). Classification and regression trees. Routledge.↩︎"
  },
  {
    "objectID": "wk6_classification1.html#challenges-overfitting",
    "href": "wk6_classification1.html#challenges-overfitting",
    "title": "6  Week6 - Classification-I",
    "section": "6.3 Challenges: Overfitting",
    "text": "6.3 Challenges: Overfitting\nOne of the significant challenges in the classification process is overfitting. Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise and outliers. As a result, the model performs exceptionally well on the training data but poorly on unseen data because it has become too complex and specific to the training examples.\nTo mitigate overfitting in decision trees, a common approach is to prune the tree, which simplifies the model by removing less significant branches(Friedman, J., 20096;Quinlan, J. R. ,2014.7). The effectiveness of this pruning process can be quantified using the Tree score formula:\n\\[Tree score = SSR + TreePenalty_{alpha}* T_{Mumber Of Leaves}\\]\nDuring the pruning process of decision trees, we use a parameter called ccp_alpha (cost complexity parameter) to control the extent of pruning. A higher value of this parameter indicates a greater tendency to remove more leaf nodes, thereby simplifying the tree structure.\nLet’s understand why removing more leaf nodes leads to a larger \\(ccp_{\\alpha}\\):\nGini Impurity and Tree Complexity\nGini impurity is a metric used to measure the purity of a dataset, with smaller values indicating higher purity(Breiman, L., 20178).\nDuring pruning, we aim to retain leaf nodes that positively contribute to the model’s performance while reducing the complexity of the model.\nWhen we remove a leaf node, the Gini impurity increases because we lose the purity associated with that leaf node.\n\n\n\nRepresentation of a decision tree before and after being processed by a pruning algorithm, where the decision nodes (light blue) classify samples into 2 classes (green and red). The red dividing lines represent the pruning step.\n\n\n\n6.3.1 Cost Complexity:\n\\(ccp_{\\alpha}\\) is a cost complexity parameter(Quinlan, J. R. ,2014.9; Breiman, L., 201710) that balances the fit and complexity of the model during pruning.\nBy increasing ccp_alpha, the model tends to remove more leaf nodes, thereby reducing the complexity of the model.\nThe purpose of this is to prevent overfitting and improve the generalization ability of the model."
  },
  {
    "objectID": "wk7_classification2.html#summary",
    "href": "wk7_classification2.html#summary",
    "title": "Week 7 - Classification and Accuracy Assessment",
    "section": "7.2 Summary",
    "text": "7.2 Summary\nBuilding upon the foundation laid in Week 6, where I explored the classification process and tackled challenges like overfitting in decision trees, this week delves deeper into the evaluation of classification models, specifically focusing on Accuracy Assessment and the Confusion Matrix.\nIn the context of remote sensing and geospatial analysis, accuracy assessment is critical to understanding how well a classification model performs. A classification model assigns labels to pixels or objects in an image based on the training data provided. However, to gauge the model’s effectiveness, we need to compare the predicted labels with the actual ground truth labels, which is where the confusion matrix comes into play."
  },
  {
    "objectID": "wk7_classification2.html#confusion-matrix-and-accuracy-metrics",
    "href": "wk7_classification2.html#confusion-matrix-and-accuracy-metrics",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.2 Confusion Matrix and Accuracy Metrics",
    "text": "7.2 Confusion Matrix and Accuracy Metrics\nA confusion matrix is a table used to evaluate the performance of a classification algorithm. It provides a detailed breakdown of the model’s predictions compared to the actual labels(Townsend, J. T., 19711). The matrix consists of four key components:\n\nTrue Positives (TP): The number of correct predictions where the model accurately classified a positive instance.\nTrue Negatives (TN): The number of correct predictions where the model accurately classified a negative instance.\nFalse Positives (FP): The number of incorrect predictions where the model incorrectly classified a negative instance as positive.\nFalse Negatives (FN): The number of incorrect predictions where the model incorrectly classified a positive instance as negative.\n\nThese metrics provide insights into the model’s performance, indicating how well it can distinguish between different classes and how reliable its predictions are."
  },
  {
    "objectID": "wk7_classification2.html#accuracy-and-misclassification",
    "href": "wk7_classification2.html#accuracy-and-misclassification",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.5 Accuracy and Misclassification",
    "text": "7.5 Accuracy and Misclassification\nWhile accuracy is a useful measure, it is important to be mindful of potential imbalances in the data, where one class may be more prevalent than others. In such cases, relying solely on accuracy can be misleading, as the model might simply be predicting the dominant class more frequently, leading to a high accuracy score but poor performance on minority classes. Therefore, other metrics like precision and recall, which focus on the model’s ability to correctly identify positive instances, become crucial."
  },
  {
    "objectID": "wk7_classification2.html#reflection-1",
    "href": "wk7_classification2.html#reflection-1",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.5 Reflection",
    "text": "7.5 Reflection\nThis reminds me of my undergraduate dissertation project, where I evaluated the accuracy of fire detection models using confusion matrices.\n\n\n\nConfusion Matrix\n\n\nFirstly, I prepared the data by creating binary classification labels using both active detection thresholding and visual inspection methods as ground truth data. Next, I trained the model using a U-net neural network model on remote sensing images, applying both cross-entropy and focal loss functions. Finally, I made predictions and evaluations by predicting fire pixel locations in new images and assessing the differences between expected and original images using confusion matrix evaluation metrics to determine prediction accuracy. Evaluating accuracy was a crucial practical experience for me in using convolutional networks to predict fire spots in remote sensing images."
  },
  {
    "objectID": "wk7_classification2.html#multi-class-confusion-matrix",
    "href": "wk7_classification2.html#multi-class-confusion-matrix",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.3 Multi-Class Confusion Matrix",
    "text": "7.3 Multi-Class Confusion Matrix\nThere are also confusion matrix for a multi-class classification task(Sammut, C. et al., 20112:), with the classes \\(A_1\\), \\(A_2\\), and \\(A_n\\). In the confusion matrix, \\(N_{ij}\\) represents the number of samples actually belonging to class \\(A_i\\) but classified as class \\(A_j\\).\n\n\n\nMulti-class classification task\n\n\nAccuracy for Multi-Class Classification\nThe overall accuracy of the classifier can be computed as:\n\\[\n\\text{Accuracy} = \\frac{\\sum_{i=1}^{n} N_{ii}}{\\sum_{i=1}^{n} \\sum_{j=1}^{n} N_{ij}}\n\\]\nWhere:\n\n\\(N_{ii}\\) is the number of correctly classified instances for class \\(A_i\\).\n\\(\\sum_{i=1}^{n} \\sum_{j=1}^{n} N_{ij}\\) is the total number of instances across all classes.\n\nPrecision for Class \\(A_i\\)\n\\[\n\\text{Precision}(A_i) = \\frac{N_{ii}}{\\sum_{j=1}^{n} N_{ji}}\n\\]\nWhere:\n\n\\(N_{ii}\\) is the number of true positives for class \\(A_i\\).\n\\(\\sum_{j=1}^{n} N_{ji}\\) is the total number of instances predicted as class \\(A_i\\) (both correctly and incorrectly).\n\nRecall for Class \\(A_i\\)\n\\[\n\\text{Recall}(A_i) = \\frac{N_{ii}}{\\sum_{j=1}^{n} N_{ij}}\n\\]\nWhere:\n\n\\(N_{ii}\\) is the number of true positives for class \\(A_i\\).\n\\(\\sum_{j=1}^{n} N_{ij}\\) is the total number of actual instances of class \\(A_i\\).\n\nF1 Score for Class \\(A_i\\)\n\\[\\text{F1Score}(A_i) = 2 \\times \\frac{\\text{Precision}(A_i) \\times \\text{Recall}(A_i)}{\\text{Precision}(A_i) + \\text{Recall}(A_i)}\\]\nThese metrics help evaluate the performance of a multi-class classification model, providing insights into how well each class is being predicted and how balanced the classifier’s performance is across all classes."
  },
  {
    "objectID": "wk7_classification2.html#improved-confusion-matrix",
    "href": "wk7_classification2.html#improved-confusion-matrix",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.4 Improved confusion matrix",
    "text": "7.4 Improved confusion matrix\nIn 2016, Xinyang Deng et al. (2016)3 proposed an improved method for constructing Basic Probability Assignment (BPA) using a confusion matrix. The method is designed to better utilize the information provided by the confusion matrix to enhance the effectiveness of BPA construction in classification problems.\n\n\n\nDiagrammatic sketch for classification problem by using Dempster–Shafer theory.\n\n\n\nUtilization of Confusion Matrix: The method leverages the recall and precision values derived from the confusion matrix to construct BPA. These values provide critical insights into the performance of each class in the classification process.\nDual Evidence Sources: Recall and precision are treated as two distinct evidence sources. The final BPA is constructed by synthesizing these two sources of evidence, allowing for a more robust representation of uncertainty in classification outcomes.\nMulti-Classifier System: The method is applicable to multi-classifier systems, where multiple classifiers are combined. The Dempster-Shafer theory is employed to fuse the information from these classifiers, enabling more accurate and reliable decision-making.\n\nThese enhancements aim to make better use of the information within the confusion matrix, improving the effectiveness of BPA construction in classification tasks."
  },
  {
    "objectID": "wk7_classification2.html#reference",
    "href": "wk7_classification2.html#reference",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.7 Reference",
    "text": "7.7 Reference"
  },
  {
    "objectID": "wk7_classification2.html#footnotes",
    "href": "wk7_classification2.html#footnotes",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "",
    "text": "Townsend, J. T. (1971). Theoretical analysis of an alphabetic confusion matrix. Perception & Psychophysics, 9, 40-50.↩︎\nSammut, C., & Webb, G. I. (Eds.). (2011). Encyclopedia of machine learning. Springer Science & Business Media.↩︎\nDeng, X., Liu, Q., Deng, Y. and Mahadevan, S., 2016. An improved method to construct basic probability assignment based on the confusion matrix for classification problem. Information Sciences, 340–341, pp.250-261. Available at: https://doi.org/10.1016/j.ins.2016.01.033↩︎"
  },
  {
    "objectID": "wk8_SAR_data.html#sar-data-analysis",
    "href": "wk8_SAR_data.html#sar-data-analysis",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.1 SAR Data Analysis",
    "text": "8.1 SAR Data Analysis\nSynthetic Aperture Radar (SAR) is a radar system that produces high-resolution images of the Earth’s surface by using the movement of the radar antenna to simulate a much larger aperture. SAR data can penetrate clouds and function during day and night, making it particularly useful in monitoring environmental and surface changes.\n\n8.1.1 Key Concepts Covered:\n\nSAR Sensors: We explored different SAR sensors, including Sentinel-1, and their capabilities in capturing radar signals.\nSAR Polarization: The ability of SAR to capture data in multiple polarizations, such as HH, HV, VV, and VH, provides deeper insights into surface characteristics.\nInterferometry: Techniques like InSAR and DInSAR were introduced, demonstrating how they are used to measure surface deformation by analyzing phase differences between radar images.\nData Fusion: We learned about combining SAR data with optical imagery to improve accuracy in classification tasks, which is especially useful in urban mapping and land cover change detection (Wang, Jun et. al, 20241).\n\n\n\n8.1.2 Difference of other SAR(Shahzad et al., 20242)\n\n\n\n\n\n\n\n\n\nSAR\nInSAR\nDInSAR\nPSInSAR\n\n\n\n\nSynthetic Aperture Radar\nInterferometric Synthetic Aperture Radar\nDifferential Interferometric Synthetic Aperture Radar\nPersistent Scatterer Interferometric SAR\n\n\n\nA radar system capable of producing high-resolution images.\nUses a “virtual” antenna length to combine echo signals received from different positions, resulting in higher-resolution radar imaging.\nUsed for surface observations such as land use, topography, and forest cover.\n\n\nAnalyzes surface deformation by exploiting the phase difference between two remote sensing images.\nCalculates the deformation at each pixel on the ground surface between two observations.\nReveals elevation and deformation information.\n\n\nBuilds upon InSAR by using phase differences from multiple remote sensing images to improve deformation measurement accuracy.\nSensitive to deformation, suitable for monitoring ground surface changes due to earthquakes, mining, landslides, etc.\nUtilizes two SAR images and external topographic data to measure subtle surface deformations.\n\n\nModels and analyzes time series data from multiple SAR images to enhance deformation inversion accuracy.\nReveals spatial distribution of surface deformations, widely used for monitoring urban subsidence and infrastructure changes."
  },
  {
    "objectID": "wk8_SAR_data.html#reflection",
    "href": "wk8_SAR_data.html#reflection",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nThis week’s exploration of SAR data and its integration with thermal infrared (TIR) data provided valuable insights into the broader applications of remote sensing for environmental monitoring. One key takeaway is the versatility of SAR in detecting surface characteristics such as roughness, moisture content, and urban structures, which, when combined with TIR data, offers a more comprehensive analysis of surface temperature variations. This multi-faceted approach is particularly useful in studying phenomena like urban heat islands and land-use changes.\nUnderstanding the combination of SAR and TIR data also highlighted the potential for improving climate change monitoring. The ability to track both land surface temperature and environmental changes in urban areas and natural landscapes makes this method crucial for addressing real-world problems such as deforestation, drought, and urbanization impacts. The class has deepened my appreciation of how fusing different types of geospatial data can lead to more precise and actionable environmental insights, reinforcing the importance of integrating multiple datasets for comprehensive analysis in future geospatial projects."
  },
  {
    "objectID": "wk8_SAR_data.html#reference",
    "href": "wk8_SAR_data.html#reference",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.4 Reference",
    "text": "8.4 Reference"
  },
  {
    "objectID": "wk8_SAR_data.html#combining-sar-data-and-thermal-infrared-data-for-surface-temperature-analysis",
    "href": "wk8_SAR_data.html#combining-sar-data-and-thermal-infrared-data-for-surface-temperature-analysis",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.2 Combining SAR Data and Thermal Infrared Data for Surface Temperature Analysis",
    "text": "8.2 Combining SAR Data and Thermal Infrared Data for Surface Temperature Analysis\nSurface temperature analysis is a key aspect of understanding environmental changes, particularly in urban areas and regions affected by climate change. By integrating Synthetic Aperture Radar (SAR) data with Thermal Infrared (TIR) data, we can achieve a more holistic understanding of surface temperature variations and their influencing factors.\n\n8.2.1 Why Combine SAR and TIR?\n\nSAR Data provides detailed insights into surface characteristics, such as roughness, moisture content, and vegetation structure. SAR is particularly useful in monitoring changes in land cover and can operate in all weather conditions, as it is not hindered by cloud cover or lack of sunlight.\nThermal Infrared Data (TIR), such as from the Landsat or MODIS sensors, allows the measurement of Land Surface Temperature (LST). TIR is essential for understanding heat dynamics in both natural and urban environments.\n\nCombining these two data types enhances our ability to:\n\nMonitor Urban Heat Islands (UHIs): By integrating LST data from TIR with SAR-based urban morphology, we can understand how factors like building density and surface materials contribute to UHI intensity.\nAssess Soil Moisture and Temperature Interactions: SAR data can be used to measure surface moisture, which directly affects surface temperature. Areas with low moisture content often exhibit higher surface temperatures, and the combination of these datasets helps quantify this relationship.\n\n\n\n8.2.2 Methodology for Integrating SAR and TIR Data\n\nData Preprocessing:\n\nConvert SAR data to a common spatial resolution with the thermal infrared data. SAR data is often available at higher spatial resolutions, so downscaling may be necessary.\nApply appropriate radiometric and geometric corrections to both SAR and TIR datasets.\n\nSurface Roughness and Temperature Analysis:\n\nSAR data can provide a roughness index of the surface, which can be correlated with temperature patterns. For example, smoother surfaces like water bodies and urban concrete will reflect heat differently than rough surfaces like forests.\n\nUrban Heat Island Detection:\n\nBy correlating LST from TIR data with building density and surface composition from SAR data, we can identify and monitor UHIs over time. This combination helps detect how specific materials and urban planning contribute to elevated temperatures in cities (Li et al., 20192).\n\nClimate Change Impact:\n\nThe combined use of SAR and TIR data allows for long-term monitoring of land-use changes and their thermal effects. As landscapes shift from vegetation to urban development, LST tends to rise. Monitoring these changes with SAR’s ability to detect deforestation and TIR’s temperature monitoring capacity provides a complete picture of land transformation impacts (Balzter et al., 20073).\n\n\n\n\n8.2.3 Applications of SAR and TIR Data Combination\n\nAgricultural Monitoring: Integrating SAR-based moisture data with thermal infrared LST data allows for the assessment of crop health. High temperatures combined with low soil moisture can indicate drought conditions, which SAR and TIR data can detect in real-time.\nDisaster Management: In the case of wildfires or heatwaves, combining SAR data with thermal imagery can help identify fire hotspots, assess damage, and predict future risks by understanding temperature patterns and fuel moisture content (Zhang et al., 20204).\n\n\n\n8.2.4 Conclusion\nThe integration of SAR and TIR data provides a comprehensive approach to analyzing surface temperature changes and their influencing factors. By leveraging the strengths of both data types, researchers and urban planners can gain deeper insights into urban heat island effects, climate change impacts, and agricultural resilience. This combined analysis can lead to more effective strategies for mitigating temperature-related risks and promoting sustainable land management."
  },
  {
    "objectID": "wk8_SAR_data.html#footnotes",
    "href": "wk8_SAR_data.html#footnotes",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "",
    "text": "Wang, Jun, Yanlong Wang, Guang Li, and Zhengyuan Qi. 2024. “Integration of Remote Sensing and Machine Learning for Precision Agriculture: A Comprehensive Perspective on Applications” Agronomy 14, no. 9: 1975. https://doi.org/10.3390/agronomy14091975↩︎\nShahzad, Naeem, and Xiaoli Ding. 2024. “An Improved Time Series SAR Interferometry (TSInSAR) for Investigating Earthquake-Induced Active Unstable Slopes (AUS) in Pakistan.” International Journal of Remote Sensing 45 (18): 6342–71. doi:10.1080/01431161.2024.2391106.↩︎\nLi, X., Zhou, Y., Zhu, Z., and Asrar, G.R., 2019. The surface urban heat island response to urban expansion: A panel analysis for the conterminous United States. Remote Sensing of Environment, 235, p.111446.↩︎\nBalzter, H., Rowland, C.S., and Saich, P., 2007. Forest canopy height and carbon estimation at Monks Wood National Nature Reserve, UK, using dual-wavelength SAR interferometry. Remote Sensing of Environment, 108(3), pp.224-239.↩︎"
  },
  {
    "objectID": "wk8_SAR_data.html#sar-data-thermal-infrared-data-for-surface-temperature-analysis",
    "href": "wk8_SAR_data.html#sar-data-thermal-infrared-data-for-surface-temperature-analysis",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.2 SAR Data & Thermal Infrared Data for Surface Temperature Analysis",
    "text": "8.2 SAR Data & Thermal Infrared Data for Surface Temperature Analysis\nSurface temperature analysis is a key aspect of understanding environmental changes, particularly in urban areas and regions affected by climate change. By integrating Synthetic Aperture Radar (SAR) data with Thermal Infrared (TIR) data, we can achieve a more holistic understanding of surface temperature variations and their influencing factors.\n\n8.2.1 Why Combine SAR and TIR?\nCombining SAR and Thermal Infrared (TIR) data provides a comprehensive view of surface temperature variations and their driving factors. SAR captures surface properties like roughness, moisture, and vegetation structure, while TIR measures Land Surface Temperature (LST). This fusion allows for deeper environmental analysis and more accurate monitoring.\nIn urban heat island (UHI) studies, SAR data reveals urban structure and morphology, while TIR highlights temperature patterns. Together, they provide clearer insights into how urban features affect heat dynamics, supporting more effective urban planning.\nAdditionally, integrating SAR’s surface moisture measurements with TIR’s temperature data helps assess the link between moisture content and surface temperature. This is crucial for monitoring drought and land degradation, offering a fuller picture of environmental conditions.\n\n\n8.2.2 Application for Integrating SAR and TIR Data\nSAR data can provide a roughness index of the surface, which can be correlated with temperature patterns. For example, smoother surfaces like water bodies and urban concrete will reflect heat differently than rough surfaces like forests.\n\nUrban Heat Island Detection:\n\nBy correlating LST from TIR data with building density and surface composition from SAR data, we can identify and monitor UHIs over time. This combination helps detect how specific materials and urban planning contribute to elevated temperatures in cities (Li et al., 20193).\n\n Where SHUI is surface urban heat island. We can observed that larger urban areas display higher SUHI than smaller urban areas, indicating the increase of SUHI with the expansion of urban area size.\nClimate Change Impact:\n\nThe combined use of SAR and TIR data allows for long-term monitoring of land-use changes and their thermal effects. As landscapes shift from vegetation to urban development, LST tends to rise. Monitoring these changes with SAR’s ability to detect deforestation and TIR’s temperature monitoring capacity provides a complete picture of land transformation impacts (Balzter et al., 20074).\n\n\n\n\n\nVegetation carbon content (t/ha) at Monks Wood NNR derived from the canopy height models from (a) LIDAR DSM and LIDAR DTM, (b) XVV InSAR DSM and LIDAR DTM, (c) XVV InSAR DSM and smoothed interpolated LHH InSAR DTM (dual wavelength approach). All images were averaged to 21 m pixel spacing as required by the Marrakech Accords to the Kyoto protocol. Color scale : 5…400 tC/ha.\n\n\nThis figure illustrates how different terrain removal approaches, using both SAR and LIDAR, can provide comprehensive insights into vegetation carbon content. As shown by Balzter et al. (2007), combining X-VV interferometry DSM with LIDAR DSM and LIDAR DTM enhances the accuracy of carbon mapping. Such integration is crucial for monitoring carbon stocks and understanding the effects of land-use changes on carbon sequestration. By combining SAR and TIR data, long-term land-use monitoring becomes more precise, enabling better assessment of the environmental impact of urbanization and deforestation."
  }
]