[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "learningDiary",
    "section": "",
    "text": "About me\nHello, my name is Xiaoyi Chen. I developed a keen interest in remote sensing during my undergraduate studies due to my involvement in a graduation project focused on remote sensing image processing. This experience sparked my curiosity and enthusiasm for exploring the intricacies of this field further.\nAdditionally, I have a passion for photography and enjoy capturing moments through my lens. Below are some of my photographic records."
  },
  {
    "objectID": "intro.html#beginning",
    "href": "intro.html#beginning",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.1 Beginning",
    "text": "1.1 Beginning\nBefore starting this course, I need to first understand the purpose of this course, which is to introduce the basic concepts and applications of remote sensing, and through this course of RS, the content we will learn includes:\n\nDefinition, classification and platform of remote perception\nInteraction of electromagnetic waves with the Earth’s surface\nFour resolutions for remote sensing data: spatial, spectral, temporal, and radiative\nFormat, selection, and limitations of remote sensing data\nPractical cases and analysis methods of remote perception data\n\nIt easily reminds me of the basic concepts of RS that I learned by myself during my undergraduate study. I believe that this course can not only help me review the past knowledge, but also fully supplement other knowledge."
  },
  {
    "objectID": "wk2_Portfolio.html",
    "href": "wk2_Portfolio.html",
    "title": "2  Presentation Ninja",
    "section": "",
    "text": "background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg)\n???\nImage credit: Wikimedia Commons\nclass: inverse, center, middle\n\n3 Get Started\n\n\n\n4 Hello World\nInstall the xaringan package from Github:\n\nremotes::install_github(\"yihui/xaringan\")\n\n–\nYou are recommended to use the RStudio IDE, but you do not have to.\n\nCreate a new R Markdown document from the menu File -&gt; New File -&gt; R Markdown -&gt; From Template -&gt; Ninja Presentation;1\n\n–\n\nClick the Knit button to compile it;\n\n–\n\nor use the RStudio Addin2 “Infinite Moon Reader” to live preview the slides (every time you update and save the Rmd document, the slides will be automatically reloaded in RStudio Viewer.\n\n.footnote[ [1] 中文用户请看这份教程\n[2] See #2 if you do not see the template or addin in RStudio. ]\n\n\n5 Hello Ninja\nAs a presentation ninja, you certainly should not be satisfied by the “Hello World” example. You need to understand more about two things:\n\nThe remark.js library;\nThe xaringan package;\n\nBasically xaringan injected the chakra of R Markdown (minus Pandoc) into remark.js. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (knitr).\n\n\n\n6 remark.js\nYou can see an introduction of remark.js from its homepage. You should read the remark.js Wiki at least once to know how to\n\ncreate a new slide (Markdown syntax* and slide properties);\nformat a slide (e.g. text alignment);\nconfigure the slideshow;\nand use the presentation (keyboard shortcuts).\n\nIt is important to be familiar with remark.js before you can understand the options in xaringan.\n.footnote[[*] It is different with Pandoc’s Markdown! It is limited but should be enough for presentation purposes. Come on… You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js may be improved in the future.]\nclass: inverse, middle, center\n\n\n7 Using xaringan\n\n\n\n8 xaringan\nProvides an R Markdown output format xaringan::moon_reader as a wrapper for remark.js, and you can use it in the YAML metadata, e.g.\n---\ntitle: \"A Cool Presentation\"\noutput:\n  xaringan::moon_reader:\n    yolo: true\n    nature:\n      autoplay: 30000\n---\nSee the help page ?xaringan::moon_reader for all possible options that you can use.\n\n\n\n9 remark.js vs xaringan\nSome differences between using remark.js (left) and using xaringan (right):\n.pull-left[ 1. Start with a boilerplate HTML file;\n\nPlain Markdown;\nWrite JavaScript to autoplay slides;\nManually configure MathJax;\nHighlight code with *;\nEdit Markdown source and refresh browser to see updated slides; ]\n\n.pull-right[ 1. Start with an R Markdown document;\n\nR Markdown (can embed R/other code chunks);\nProvide an option autoplay;\nMathJax just works;*\nHighlight code with {{}};\nThe RStudio addin “Infinite Moon Reader” automatically refreshes slides on changes; ]\n\n.footnote[[*] Not really. See next page.]\n\n\n\n10 Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $+$ renders \\(\\alpha+\\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $.\nMath does not work on the title slide (see #61 for a workaround).\n\n\n\n\n11 R Code\n\n# a boring regression\nfit = lm(dist ~ 1 + speed, data = cars)\ncoef(summary(fit))\n\n#               Estimate Std. Error   t value     Pr(&gt;|t|)\n# (Intercept) -17.579095  6.7584402 -2.601058 1.231882e-02\n# speed         3.932409  0.4155128  9.463990 1.489836e-12\n\ndojutsu = c('地爆天星', '天照', '加具土命', '神威', '須佐能乎', '無限月読')\ngrep('天', dojutsu, value = TRUE)\n\n# [1] \"地爆天星\" \"天照\"\n\n\n\n\n\n12 R Plots\n\npar(mar = c(4, 4, 1, .1))\nplot(cars, pch = 19, col = 'darkgray', las = 1)\nabline(fit, lwd = 2)\n\n\n\n\n\n\n\n13 Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\nknitr::kable(head(iris), format = 'html')\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\n\n\n\n14 HTML Widgets\nI have not thoroughly tested HTML widgets against xaringan. Some may work well, and some may not. It is a little tricky.\nSimilarly, the Shiny mode (runtime: shiny) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications.\nSee the next page for two HTML widgets.\n\n\nlibrary(leaflet)\nleaflet() %&gt;% addTiles() %&gt;% setView(-93.65, 42.0285, zoom = 17)\n\n\n\n\n\n\n\nDT::datatable(\n  head(iris, 10),\n  fillContainer = FALSE, options = list(pageLength = 8)\n)\n\n\n\n\n15 Some Tips\n\nDo not forget to try the yolo option of xaringan::moon_reader.\noutput:\n  xaringan::moon_reader:\n    yolo: true\n\n\n\n\n16 Some Tips\n\nSlides can be automatically played if you set the autoplay option under nature, e.g. go to the next slide every 30 seconds in a lightning talk:\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay: 30000\nIf you want to restart the play after it reaches the last slide, you may set the sub-option loop to TRUE, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay:\n        interval: 30000\n        loop: true\n\n\n\n\n17 Some Tips\n\nA countdown timer can be added to every page of the slides using the countdown option under nature, e.g. if you want to spend one minute on every page when you give the talk, you can set:\noutput:\n  xaringan::moon_reader:\n    nature:\n      countdown: 60000\nThen you will see a timer counting down from 01:00, to 00:59, 00:58, … When the time is out, the timer will continue but the time turns red.\n\n\n\n\n18 Some Tips\n\nThe title slide is created automatically by xaringan, but it is just another remark.js slide added before your other slides.\nThe title slide is set to class: center, middle, inverse, title-slide by default. You can change the classes applied to the title slide with the titleSlideClass option of nature (title-slide is always applied).\noutput:\n  xaringan::moon_reader:\n    nature:\n      titleSlideClass: [top, left, inverse]\n\n–\n\nIf you’d like to create your own title slide, disable xaringan’s title slide with the seal = FALSE option of moon_reader.\noutput:\n  xaringan::moon_reader:\n    seal: false\n\n\n\n\n19 Some Tips\n\nThere are several ways to build incremental slides. See this presentation for examples.\nThe option highlightLines: true of nature will highlight code lines that start with *, or are wrapped in {{ }}, or have trailing comments #&lt;&lt;;\noutput:\n  xaringan::moon_reader:\n    nature:\n      highlightLines: true\nSee examples on the next page.\n\n\n\n\n20 Some Tips\n.pull-left[ An example using a leading *:\n```r\nif (TRUE) {\n** message(\"Very important!\")\n}\n```\nOutput:\nif (TRUE) {\n* message(\"Very important!\")\n}\nThis is invalid R code, so it is a plain fenced code block that is not executed. ]\n.pull-right[ An example using {{}}:\n```{r tidy=FALSE}\nif (TRUE) {\n*{{ message(\"Very important!\") }}\n}\n```\nOutput:\n\nif (TRUE) {\n{{ message(\"Very important!\") }}\n}\n\nVery important!\n\n\nIt is valid R code so you can run it. Note that {{}} can wrap an R expression of multiple lines. ]\n\n\n\n21 Some Tips\nAn example of using the trailing comment #&lt;&lt; to highlight lines:\n```{r tidy=FALSE}\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #&lt;&lt;\n  geom_smooth()    #&lt;&lt;\n```\nOutput:\n\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #&lt;&lt;\n  geom_smooth()    #&lt;&lt;\n\n\n\n\n22 Some Tips\nWhen you enable line-highlighting, you can also use the chunk option highlight.output to highlight specific lines of the text output from a code chunk. For example, highlight.output = TRUE means highlighting all lines, and highlight.output = c(1, 3) means highlighting the first and third line.\n```{r, highlight.output=c(1, 3)}\nhead(iris)\n```\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nQuestion: what does highlight.output = c(TRUE, FALSE) mean? (Hint: think about R’s recycling of vectors)\n\n\n\n23 Some Tips\n\nTo make slides work offline, you need to download a copy of remark.js in advance, because xaringan uses the online version by default (see the help page ?xaringan::moon_reader).\nYou can use xaringan::summon_remark() to download the latest or a specified version of remark.js. By default, it is downloaded to libs/remark-latest.min.js.\nThen change the chakra option in YAML to point to this file, e.g.\noutput:\n  xaringan::moon_reader:\n    chakra: libs/remark-latest.min.js\nIf you used Google fonts in slides (the default theme uses Yanone Kaffeesatz, Droid Serif, and Source Code Pro), they won’t work offline unless you download or install them locally. The Heroku app google-webfonts-helper can help you download fonts and generate the necessary CSS.\n\n\n\n\n24 Macros\n\nremark.js allows users to define custom macros (JS functions) that can be applied to Markdown text using the syntax ![:macroName arg1, arg2, ...] or ![:macroName arg1, arg2, ...](this). For example, before remark.js initializes the slides, you can define a macro named scale:\nremark.macros.scale = function (percentage) {\n  var url = this;\n  return '&lt;img src=\"' + url + '\" style=\"width: ' + percentage + '\" /&gt;';\n};\nThen the Markdown text\n![:scale 50%](image.jpg)\nwill be translated to\n&lt;img src=\"image.jpg\" style=\"width: 50%\" /&gt;\n\n\n\n\n25 Macros (continued)\n\nTo insert macros in xaringan slides, you can use the option beforeInit under the option nature, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      beforeInit: \"macros.js\"\nYou save your remark.js macros in the file macros.js.\nThe beforeInit option can be used to insert arbitrary JS code before remark.create(). Inserting macros is just one of its possible applications.\n\n\n\n\n26 CSS\nAmong all options in xaringan::moon_reader, the most challenging but perhaps also the most rewarding one is css, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know.\nYou can see the default CSS file here. You can completely replace it with your own CSS files, or define new rules to override the default. See the help page ?xaringan::moon_reader for more information.\n\n\n\n27 CSS\nFor example, suppose you want to change the font for code from the default “Source Code Pro” to “Ubuntu Mono”. You can create a CSS file named, say, ubuntu-mono.css:\n@import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);\n\n.remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }\nThen set the css option in the YAML metadata:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"ubuntu-mono.css\"]\nHere I assume ubuntu-mono.css is under the same directory as your Rmd.\nSee yihui/xaringan#83 for an example of using the Fira Code font, which supports ligatures in program code.\n\n\n\n28 CSS (with Sass)\nxaringan also supports Sass support via rmarkdown. Suppose you want to use the same color for different elements, e.g., first heading and bold text. You can create a .scss file, say mytheme.scss, using the sass syntax with variables:\n$mycolor: #ff0000; \n.remark-slide-content &gt; h1 { color: $mycolor; }\n.remark-slide-content strong { color: $mycolor; }\nThen set the css option in the YAML metadata using this file placed under the same directory as your Rmd:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"mytheme.scss\"]\nThis requires rmarkdown &gt;= 2.8 and the sass package. You can learn more about rmarkdown and sass support in this blog post and in sass overview vignette.\n\n\n\n29 Themes\nDon’t want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files foo.css and foo-fonts.css, where foo is the theme name. Below are some existing themes:\n\nnames(xaringan:::list_css())\n\n [1] \"chocolate-fonts\"  \"chocolate\"        \"default-fonts\"   \n [4] \"default\"          \"duke-blue\"        \"fc-fonts\"        \n [7] \"fc\"               \"glasgow_template\" \"hygge-duke\"      \n[10] \"hygge\"            \"ki-fonts\"         \"ki\"              \n[13] \"kunoichi\"         \"lucy-fonts\"       \"lucy\"            \n[16] \"metropolis-fonts\" \"metropolis\"       \"middlebury-fonts\"\n[19] \"middlebury\"       \"nhsr-fonts\"       \"nhsr\"            \n[22] \"ninjutsu\"         \"rladies-fonts\"    \"rladies\"         \n[25] \"robot-fonts\"      \"robot\"            \"rutgers-fonts\"   \n[28] \"rutgers\"          \"shinobi\"          \"tamu-fonts\"      \n[31] \"tamu\"             \"uio-fonts\"        \"uio\"             \n[34] \"uo-fonts\"         \"uo\"               \"uol-fonts\"       \n[37] \"uol\"              \"useR-fonts\"       \"useR\"            \n[40] \"uwm-fonts\"        \"uwm\"              \"wic-fonts\"       \n[43] \"wic\"             \n\n\n\n\n\n30 Themes\nTo use a theme, you can specify the css option as an array of CSS filenames (without the .css extensions), e.g.,\noutput:\n  xaringan::moon_reader:\n    css: [default, metropolis, metropolis-fonts]\nIf you want to contribute a theme to xaringan, please read this blog post.\nbackground-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) background-size: 100px background-position: 90% 8%\n\n\n31 Sharingan\nThe R package name xaringan was derived1 from Sharingan, a dōjutsu in the Japanese anime Naruto with two abilities:\n\nthe “Eye of Insight”\nthe “Eye of Hypnotism”\n\nI think a presentation is basically a way to communicate insights to the audience, and a great presentation may even “hypnotize” the audience.2,3\n.footnote[ [1] In Chinese, the pronounciation of X is Sh /ʃ/ (as in shrimp). Now you should have a better idea of how to pronounce my last name Xie.\n[2] By comparison, bad presentations only put the audience to sleep.\n[3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations. ]\n\n\n\n32 Naruto terminology\nThe xaringan package borrowed a few terms from Naruto, such as\n\nSharingan (写輪眼; the package name)\nThe moon reader (月読; an attractive R Markdown output format)\nChakra (查克拉; the path to the remark.js library, which is the power to drive the presentation)\nNature transformation (性質変化; transform the chakra by setting different options)\nThe infinite moon reader (無限月読; start a local web server to continuously serve your slides)\nThe summoning technique (download remark.js from the web)\n\nYou can click the links to know more about them if you want. The jutsu “Moon Reader” may seem a little evil, but that does not mean your slides are evil.\n\nclass: center\n\n\n33 Hand seals (印)\nPress h or ? to see the possible ninjutsu you can use in remark.js.\n\n\nclass: center, middle\n\n\n34 Thanks!\nSlides created via the R package xaringan.\nThe chakra comes from remark.js, knitr, and R Markdown."
  },
  {
    "objectID": "wk5_GEE.html#summary",
    "href": "wk5_GEE.html#summary",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThe theoretical part of the course mainly introduced the basic concepts, data structures, operations, and application cases of Google Earth Engine (GEE).\nCharacteristics of GEE: GEE is a geospatial processing service that utilizes cloud servers to store and analyze massive amounts of remote sensing imagery and geographic data, enabling rapid and large-scale monitoring and simulation of changes on the Earth’s surface.\nData structure of GEE: Data in GEE is divided into two types: Images and Features, corresponding to raster and vector data, respectively. Images and Features can form Collections, representing stacks of multiple images or features. Data in GEE exists in the form of objects, each with its own properties and methods.\nOperations in GEE: GEE uses the JavaScript language for coding, which can be run in the browser. The code in GEE is divided into client-side and server-side, with the client-side code mainly used for interface control and interaction, and the server-side code mainly used for data processing and analysis. Data processing in GEE mainly relies on reducers, which can perform various statistical, analytical, and transformation operations on images or features. GEE also provides some advanced features such as regression, joining, and machine learning."
  },
  {
    "objectID": "wk5_GEE.html#whats-gee",
    "href": "wk5_GEE.html#whats-gee",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.2 What’s GEE",
    "text": "5.2 What’s GEE"
  },
  {
    "objectID": "wk7_classification1.html#overfiting",
    "href": "wk7_classification1.html#overfiting",
    "title": "4  Week7 - Classification-I",
    "section": "4.1 Overfiting",
    "text": "4.1 Overfiting\n\nTree score = SSR + tree penalty (alpha) * T (number of leaves)\n\n对于Tree score公式，为啥去掉越多leaf, alpha越大？\n在决策树的剪枝过程中，我们使用一个参数称为ccp_alpha（cost complexity parameter）来控制剪枝的程度。这个参数的值越大，意味着我们更倾向于剪掉更多的叶节点，从而简化树的结构。\n让我们来理解一下为什么去掉越多的叶节点，ccp_alpha 就越大：\n\n基尼杂质和树的复杂度：\n\n基尼杂质是用来衡量数据集纯度的指标，它越小表示数据集的纯度越高。\n在剪枝过程中，我们希望保留那些对模型性能有积极影响的叶节点，同时减少模型的复杂度。\n当我们去掉一个叶节点时，基尼杂质会增加，因为我们丧失了这个叶节点的纯度。\n\n成本复杂度：\n\nccp_alpha 是一个成本复杂度参数，它在剪枝过程中平衡了模型的拟合程度和复杂度。\n当我们增大 ccp_alpha 时，模型更倾向于剪掉更多的叶节点，从而减少模型的复杂度。\n这样做的目的是防止过拟合，提高模型的泛化能力。\nYoutube video\nHow to Prune Regression Trees"
  },
  {
    "objectID": "wk7_classification1.html#random-forest",
    "href": "wk7_classification1.html#random-forest",
    "title": "4  Week7 - Classification-I",
    "section": "4.2 Random Forest",
    "text": "4.2 Random Forest"
  },
  {
    "objectID": "wk9_SAR_data.html#abstract",
    "href": "wk9_SAR_data.html#abstract",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.1 Abstract",
    "text": "8.1 Abstract\nThis class introduces the basic principle of synthetic aperture radar (SAR), two sensor types, SAR data background, SAR data value, SAR polarization, SAR background, differential interferometric synthetic aperture radar (DlnSAR), SAR data processing, SAR data fusion, SAR image fusion and other concepts. This paper also introduces how to use SAR data for change detection, including statistical test, threshold screening and ROC curve, image fusion and so on. Finally, a change detection algorithm based on SAR data is proposed, and an example is given."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "9  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "intro.html#summary-of-week-1-class",
    "href": "intro.html#summary-of-week-1-class",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.2 Summary of week 1 class",
    "text": "1.2 Summary of week 1 class\n\n远程感知城市和环境：这是一门介绍远程感知基本原理和应用的课程，主要关注城市和环境问题。\n远程感知的类型和原理：远程感知分为主动和被动两种，根据是否有自身的能源发射器。远程感知数据受到电磁波与大气和地表的相互作用的影响，需要进行校正和处理。\n远程感知数据的格式和分辨率：远程感知数据通常是栅格数据，有不同的存储格式和组织方式。远程感知数据的质量和应用受到空间、光谱、时间和辐射分辨率的制约。"
  },
  {
    "objectID": "intro.html#reflection",
    "href": "intro.html#reflection",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.4 Reflection",
    "text": "1.4 Reflection\nThrough the basic knowledge of this class, I inadvertently deepened my understanding of Landsat series satellites from the concepts of electromagnetic wave, multispectral image and hyperspectral image. Landsat provides the longest continuous space record of Earth’s land mass. Its data is essential for us to make informed decisions about the Earth’s resources and environment. It Landsat is more than just a camera circling the globe with an excellent zoom lens. It measures how much light the Earth reflects from the sun. With Landsat, we have access to a variety of useful images that reveal more about the Earth and help us better understand and manage our planet. For example, these images can be used to monitor the effects of natural and human factors such as land cover, climate change, urbanization, drought, fires, changes in biomass, etc. Therefore, the public use of Landsat data makes a lot of sense, and experts, scholars, and enterprise engineers can implement projects according to their needs."
  },
  {
    "objectID": "intro.html#summary-of-this-class",
    "href": "intro.html#summary-of-this-class",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.1 Summary of this class",
    "text": "1.1 Summary of this class\n\n\n\n\n\n\nSummary\n\n\n\n\nTele-sensing Cities and Environments: This is a course that introduces the basic principles and applications of tele-sensing, with a focus on urban and environmental issues.\nTypes and principles of remote sensing: Remote sensing is divided into active and passive, depending on whether it has its own energy transmitter. Remote sensing data is affected by the interaction of electromagnetic waves with the atmosphere and the surface, and needs to be corrected and processed.\nFormat and resolution of remote sensing data: Remote sensing data is usually raster data and has different storage formats and ways of organizing. The quality and application of remote sensing data are constrained by spatial, spectral, temporal and radiative resolution.\n\n\n\nIt easily reminds me of the basic concepts of RS that I learned by myself during my undergraduate study. I believe that this course can not only help me review the past knowledge, but also fully supplement other knowledge."
  },
  {
    "objectID": "intro.html#遥感的基本概念与原理",
    "href": "intro.html#遥感的基本概念与原理",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.3 遥感的基本概念与原理",
    "text": "1.3 遥感的基本概念与原理\n\n1.3.1 被动传感器与主动传感器的区别\n两者的区别在于：被动传感器受大气散射的影响，需要在光照条件良好的时候工作，而主动传感器可以穿透云层、火山灰和大气条件，可以在夜间工作。\n我能找到的关于主动传感器的例子是合成孔径雷达（SAR），可以“看穿云层”，并且具有极化特性，可以根据表面的粗糙度、形状、方向、湿度、盐度、密度等反映出不同的电磁波。\n\n\n1.3.2 关于电磁波"
  },
  {
    "objectID": "wk9_SAR_data.html#difference-of-other-sar",
    "href": "wk9_SAR_data.html#difference-of-other-sar",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.2 Difference of other SAR",
    "text": "8.2 Difference of other SAR\n\n\n\n\n\n\n\n\n\nSAR\nInSAR\nDInSAR\nPSInSAR\n\n\n\n\nSynthetic Aperture Radar\nInterferometric Synthetic Aperture Radar\nDifferential Interferometric Synthetic Aperture Radar\nPersistent Scatterer Interferometric SAR\n\n\n\nA radar system capable of producing high-resolution images.\nUses a “virtual” antenna length to combine echo signals received from different positions, resulting in higher-resolution radar imaging.\nUsed for surface observations such as land use, topography, and forest cover.\n\n\nAnalyzes surface deformation by exploiting the phase difference between two remote sensing images.\nCalculates the deformation at each pixel on the ground surface between two observations.\nReveals elevation and deformation information.\n\n\nBuilds upon InSAR by using phase differences from multiple remote sensing images to improve deformation measurement accuracy.\nSensitive to deformation, suitable for monitoring ground surface changes due to earthquakes, mining, landslides, etc.\nUtilizes two SAR images and external topographic data to measure subtle surface deformations.\n\n\nModels and analyzes time series data from multiple SAR images to enhance deformation inversion accuracy.\nReveals spatial distribution of surface deformations, widely used for monitoring urban subsidence and infrastructure changes."
  },
  {
    "objectID": "wk9_SAR_data.html#sar数据的应用",
    "href": "wk9_SAR_data.html#sar数据的应用",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.3 SAR数据的应用",
    "text": "8.3 SAR数据的应用\n分析两个图像之间的变化（例如比率或对数比率）\nOpen Access Damage Detection Using Sentinel-1 Imagery\nBlast Damage Assessment\n通过以下方式查看随时间变化的差异：\nPixel-Wise T-Test"
  },
  {
    "objectID": "intro.html#basic-concepts-and-principles-of-remote-sensing",
    "href": "intro.html#basic-concepts-and-principles-of-remote-sensing",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.2 Basic concepts and principles of remote sensing",
    "text": "1.2 Basic concepts and principles of remote sensing\n\n1.2.1 The difference between passive and active sensors\nThe difference between the two is that passive sensors are affected by atmospheric scattering and need to work when light conditions are good, while active sensors can penetrate clouds, volcanic ash and atmospheric conditions and can work at night.\n\n\n\nActive remote sensing example\nPassive remote sensing example\n\n\n\n\nFor the untrained eye, it’s just a bunch of black and white pixels. But the reality is that there’s more than meets the eye. For example, the 3 main types of backscatter are: Specular reflection Double-bounce Diffuse scattering\nPassive remote sensing can be very similar to how our eyes interpret the world. But the power of passive remote sensing is to see light in the whole electromagnetic spectrum. For example, this multispectral image can have different band combinations like color infrared.\n\n\nAn example of an active sensor is synthetic Aperture radar (SAR), which can “see through the clouds” and has polarization characteristics that can reflect different electromagnetic waves based on surface roughness, shape, orientation, humidity, salinity, density, etc.\nIn terms of passive remote sensing, the Landsat mission is the longest-running earth observation program. On board Landsat-8, OLI generates 9 spectral bands (Band 1 to 9).\n\n\n\n\na SAR image\n\nRocky Mountains in true color\n\n\n\n\n\n1.2.2 The relation between electromagnetic wave and multispectrum\nAccording to the frequency of electromagnetic wave vibration, the electromagnetic spectrum can be divided into visible spectrum and invisible electromagnetic spectrum two parts.\nMultispectral remote sensing refers to the remote sensing observation and research of ground targets in multiple spectral bands. These bands can include infrared, visible, near-infrared, etc. By analyzing the information of these different bands, more information about the ground target can be obtained.\n\n\n\nMultispectral common channels\n\n\n\n\n\nComparison of spectral bands between Sentinel-2 and Landsat-81.\n\n\nIn general, the relationship between electromagnetic wave and multi-spectrum is that multi-spectral remote sensing uses different bands of electromagnetic wave, and more information of ground targets can be obtained by analyzing the information of these bands."
  },
  {
    "objectID": "intro.html#reference",
    "href": "intro.html#reference",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.5 Reference",
    "text": "1.5 Reference\nLópez-Puigdollers D, Mateo-García G, Gómez-Chova L. Benchmarking Deep Learning Models for Cloud Detection in Landsat-8 and Sentinel-2 Images. Remote Sensing. 2021; 13(5):992. https://doi.org/10.3390/rs13050992\n\n\n\n\nLópez-Puigdollers, Dan, Gonzalo Mateo-García, and Luis Gómez-Chova. 2021. “Benchmarking Deep Learning Models for Cloud Detection in Landsat-8 and Sentinel-2 Images.” Remote Sensing 13 (5): 992. https://doi.org/10.3390/rs13050992."
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Week1 - Introduction of RS",
    "section": "",
    "text": "López-Puigdollers D, Mateo-García G, Gómez-Chova L. Benchmarking Deep Learning Models for Cloud Detection in Landsat-8 and Sentinel-2 Images. Remote Sensing. 2021; 13(5):992. https://doi.org/10.3390/rs13050992↩︎\nSingh, D., Dohal, S., Yadav, G., Pandey, H., & Singh, V. (2023). Utilizing GIS and remote sensing methods for polluted water body remediation: A deeper understanding. Sustainable Systems Science: Geospatial Methods and Modeling (Vol. 1, pp. 155-169). https://doi.org/10.1016/B978-0-323-91880-0.00021-0↩︎\nYu, H., Kong, B., Wang, Q., Liu, X. & Liu, X. (2020). Hyperspectral remote sensing applications in soil: a review1. In Earth Observation, (pp. 269-291). DOI: &lt;10.1016/B978-0-08-102894-0.00011-5&gt;↩︎"
  },
  {
    "objectID": "wk2_xaringen.html",
    "href": "wk2_xaringen.html",
    "title": "2  Week 2 - Xaringen",
    "section": "",
    "text": "3 Get Started\nInstall the xaringan package from Github:\n{r eval=FALSE, tidy=FALSE} remotes::install_github(\"yihui/xaringan\")\n–\nYou are recommended to use the RStudio IDE, but you do not have to.\n–\n–\n.footnote[ [1] 中文用户请看这份教程\n[2] See #2 if you do not see the template or addin in RStudio. ]\nAs a presentation ninja, you certainly should not be satisfied by the “Hello World” example. You need to understand more about two things:\nBasically xaringan injected the chakra of R Markdown (minus Pandoc) into remark.js. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (knitr).\nYou can see an introduction of remark.js from its homepage. You should read the remark.js Wiki at least once to know how to\nIt is important to be familiar with remark.js before you can understand the options in xaringan.\n.footnote[[*] It is different with Pandoc’s Markdown! It is limited but should be enough for presentation purposes. Come on… You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js may be improved in the future.]\nclass: inverse, middle, center\nProvides an R Markdown output format xaringan::moon_reader as a wrapper for remark.js, and you can use it in the YAML metadata, e.g.\nSee the help page ?xaringan::moon_reader for all possible options that you can use.\nSome differences between using remark.js (left) and using xaringan (right):\n.pull-left[ 1. Start with a boilerplate HTML file;\n.pull-right[ 1. Start with an R Markdown document;\n.footnote[[*] Not really. See next page.]\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $+$ renders \\(\\alpha+\\beta\\). You can use the display style with double dollar signs:\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nLimitations:\n{r comment='#'} # a boring regression fit = lm(dist ~ 1 + speed, data = cars) coef(summary(fit)) dojutsu = c('地爆天星', '天照', '加具土命', '神威', '須佐能乎', '無限月読') grep('天', dojutsu, value = TRUE)\n{r cars, fig.height=4, dev='svg'} par(mar = c(4, 4, 1, .1)) plot(cars, pch = 19, col = 'darkgray', las = 1) abline(fit, lwd = 2)\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\nI have not thoroughly tested HTML widgets against xaringan. Some may work well, and some may not. It is a little tricky.\nSimilarly, the Shiny mode (runtime: shiny) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications.\nSee the next page for two HTML widgets.\n{r out.width='100%', fig.height=6, eval=require('leaflet')} library(leaflet) leaflet() %&gt;% addTiles() %&gt;% setView(-93.65, 42.0285, zoom = 17)\n{r eval=require('DT'), tidy=FALSE} DT::datatable(   head(iris, 10),   fillContainer = FALSE, options = list(pageLength = 8) )\n–\n.pull-left[ An example using a leading *:\nOutput:\nThis is invalid R code, so it is a plain fenced code block that is not executed. ]\n.pull-right[ An example using {{}}:\nOutput:\n{r tidy=FALSE} if (TRUE) { {{ message(\"Very important!\") }} }\nIt is valid R code so you can run it. Note that {{}} can wrap an R expression of multiple lines. ]\nAn example of using the trailing comment #&lt;&lt; to highlight lines:\nOutput:\n{r tidy=FALSE, eval=FALSE} library(ggplot2) ggplot(mtcars) +    aes(mpg, disp) +    geom_point() +   #&lt;&lt;   geom_smooth()    #&lt;&lt;\nWhen you enable line-highlighting, you can also use the chunk option highlight.output to highlight specific lines of the text output from a code chunk. For example, highlight.output = TRUE means highlighting all lines, and highlight.output = c(1, 3) means highlighting the first and third line.\n{r, highlight.output=c(1, 3), echo=FALSE} head(iris)\nQuestion: what does highlight.output = c(TRUE, FALSE) mean? (Hint: think about R’s recycling of vectors)\nAmong all options in xaringan::moon_reader, the most challenging but perhaps also the most rewarding one is css, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know.\nYou can see the default CSS file here. You can completely replace it with your own CSS files, or define new rules to override the default. See the help page ?xaringan::moon_reader for more information.\nFor example, suppose you want to change the font for code from the default “Source Code Pro” to “Ubuntu Mono”. You can create a CSS file named, say, ubuntu-mono.css:\nThen set the css option in the YAML metadata:\nHere I assume ubuntu-mono.css is under the same directory as your Rmd.\nSee yihui/xaringan#83 for an example of using the Fira Code font, which supports ligatures in program code.\nxaringan also supports Sass support via rmarkdown. Suppose you want to use the same color for different elements, e.g., first heading and bold text. You can create a .scss file, say mytheme.scss, using the sass syntax with variables:\nThen set the css option in the YAML metadata using this file placed under the same directory as your Rmd:\nThis requires rmarkdown &gt;= 2.8 and the sass package. You can learn more about rmarkdown and sass support in this blog post and in sass overview vignette.\nDon’t want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files foo.css and foo-fonts.css, where foo is the theme name. Below are some existing themes:\n{r, R.options=list(width = 70)} names(xaringan:::list_css())\nTo use a theme, you can specify the css option as an array of CSS filenames (without the .css extensions), e.g.,\nIf you want to contribute a theme to xaringan, please read this blog post.\nbackground-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) background-size: 100px background-position: 90% 8%\nThe R package name xaringan was derived1 from Sharingan, a dōjutsu in the Japanese anime Naruto with two abilities:\nI think a presentation is basically a way to communicate insights to the audience, and a great presentation may even “hypnotize” the audience.2,3\n.footnote[ [1] In Chinese, the pronounciation of X is Sh /ʃ/ (as in shrimp). Now you should have a better idea of how to pronounce my last name Xie.\n[2] By comparison, bad presentations only put the audience to sleep.\n[3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations. ]\nThe xaringan package borrowed a few terms from Naruto, such as\nYou can click the links to know more about them if you want. The jutsu “Moon Reader” may seem a little evil, but that does not mean your slides are evil.\nclass: center\nPress h or ? to see the possible ninjutsu you can use in remark.js.\nclass: center, middle\nSlides created via the R package xaringan.\nThe chakra comes from remark.js, knitr, and R Markdown. ```"
  },
  {
    "objectID": "wk2_xaringen.html#abstract",
    "href": "wk2_xaringen.html#abstract",
    "title": "2  Week 2 - Xaringen",
    "section": "2.1 Abstract",
    "text": "2.1 Abstract"
  },
  {
    "objectID": "wk3_RS_data.html#abstract",
    "href": "wk3_RS_data.html#abstract",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.1 Abstract",
    "text": "3.1 Abstract\n\n\n\n\n\n\nRemote Sensing Data\n\n\n\nThis lecture mainly provides detailed information on remote sensing cities and environments, especially on the use and processing of remote sensing data, and introduces the correction methods of remote sensing data, including geometric, atmospheric, orthographic/topographic correction and radiometric correction. Prior to this, data processing is required, including data linking and enhancement such as feathering, image enhancement, and other enhancement techniques. In addition to this, this lesson introduces remote sensing technology, explaining the different types of remote sensing sensors, such as push-sweep and scan sensors, and how they work."
  },
  {
    "objectID": "wk4_policy.html#abstract",
    "href": "wk4_policy.html#abstract",
    "title": "4  Week4 - Policy Applications",
    "section": "4.1 Abstract",
    "text": "4.1 Abstract"
  },
  {
    "objectID": "wk6_classification1.html#overfiting",
    "href": "wk6_classification1.html#overfiting",
    "title": "6  Week7 - Classification-I",
    "section": "6.1 Overfiting",
    "text": "6.1 Overfiting\n\nTree score = SSR + tree penalty (alpha) * T (number of leaves)\n\n对于Tree score公式，为啥去掉越多leaf, alpha越大？\n在决策树的剪枝过程中，我们使用一个参数称为ccp_alpha（cost complexity parameter）来控制剪枝的程度。这个参数的值越大，意味着我们更倾向于剪掉更多的叶节点，从而简化树的结构。\n让我们来理解一下为什么去掉越多的叶节点，ccp_alpha 就越大：\n\n基尼杂质和树的复杂度：\n\n基尼杂质是用来衡量数据集纯度的指标，它越小表示数据集的纯度越高。\n在剪枝过程中，我们希望保留那些对模型性能有积极影响的叶节点，同时减少模型的复杂度。\n当我们去掉一个叶节点时，基尼杂质会增加，因为我们丧失了这个叶节点的纯度。\n\n成本复杂度：\n\nccp_alpha 是一个成本复杂度参数，它在剪枝过程中平衡了模型的拟合程度和复杂度。\n当我们增大 ccp_alpha 时，模型更倾向于剪掉更多的叶节点，从而减少模型的复杂度。\n这样做的目的是防止过拟合，提高模型的泛化能力。\nYoutube video\nHow to Prune Regression Trees"
  },
  {
    "objectID": "wk6_classification1.html#random-forest",
    "href": "wk6_classification1.html#random-forest",
    "title": "6  Week7 - Classification-I",
    "section": "6.2 Random Forest",
    "text": "6.2 Random Forest"
  },
  {
    "objectID": "wk7_classification2.html#abstract",
    "href": "wk7_classification2.html#abstract",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.1 Abstract",
    "text": "7.1 Abstract\nHere is the overview of remote sensing techniques for classifying and assessing the accuracy of land cover data. Here are the key points:\n\nLandcover Classification: It discusses the use of pre-classified data sources like GlobeLand30, ESA’s CCI, Dynamic World, MODIS, and Google building data for landcover classification.\nDynamic World: The page details the process of training, pre-processing, normalization, and classification using CNNs, with a focus on Dynamic World’s semi-supervised approach and regional division for sample stratification.\nSub Pixel Analysis: It explains the concept of sub pixel classification, spectral mixture analysis, and linear spectral unmixing, including mathematical formulas for calculating the proportion of landcover per pixel.\nAccuracy Assessment: The page outlines various accuracy assessment methods in remote sensing, such as producer’s accuracy, user’s accuracy, overall accuracy, and the Kappa coefficient, along with their definitions and significance."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "López-Puigdollers, Dan, Gonzalo Mateo-García, and Luis Gómez-Chova.\n2021. “Benchmarking Deep Learning Models for Cloud Detection in\nLandsat-8 and Sentinel-2 Images.” Remote Sensing 13 (5):\n992. https://doi.org/10.3390/rs13050992."
  },
  {
    "objectID": "wk2_xaringen.html#xaringen",
    "href": "wk2_xaringen.html#xaringen",
    "title": "2  Week 2 - Xaringen",
    "section": "2.1 Xaringen",
    "text": "2.1 Xaringen"
  },
  {
    "objectID": "wk3_RS_data.html#summary-of-common-remote-sensing-indices",
    "href": "wk3_RS_data.html#summary-of-common-remote-sensing-indices",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.2 Summary of Common Remote Sensing Indices",
    "text": "3.2 Summary of Common Remote Sensing Indices\nThe various techniques, including atmosphereic correction and imagery merging, used in remote sensing to enhance and process imagery data share common principles. While they serve different purposes1, they share some common principles:\n\nData Transformation: Most of these techniques involve transforming the raw data to enhance its usability and interpretability. This transformation can be radiometric, spatial, or spectral.\nEnhancement of Information: These methods aim to enhance the quality of the data to make it more useful for analysis. This could involve reducing noise, highlighting certain features, or combining data from different sources.\nMathematical and Statistical Methods: Many of these techniques rely on mathematical and statistical methods to process the data. This includes algorithms for filtering, statistical analysis for PCA, and mathematical models for atmospheric correction.\nImprovement of Visualization: Techniques such as enhancement and filtering are often used to improve the visualization of imagery, making it easier to interpret and analyze.\n\n\n\n\n\n\nRemote sensing process and ‘Remote Sensing for Earth Observation’ book chapters(Klaus Temfli et al., 2009)2\nIn remote sensing analysis, various indices are commonly used to extract information about land cover and land use. Here is a summary of some common remote sensing indices:\n\n\n\nSummary of Common Remote Sensing Indices\n\n\nIndex\nDescription\nFormula\n\n\n\n\nNormalized Difference Vegetation Index (NDVI)\nUsed to assess vegetation health and density by comparing the difference between near-infrared (NIR) and red band reflectance.\n$$NDVI= \\frac{NIR-Red}{NIR+Red}$$\n\n\nNormalized Difference Water Index (NDWI)\nUtilized to identify water bodies by comparing the difference between green and near-infrared band reflectance.\n$$NDWI = \\frac{(Green - NIR)}{(Green + NIR)}$$\n\n\nNormalized Difference Built-up Index (NDBI)\nEmployed to detect built-up areas by comparing the difference between near-infrared and shortwave infrared band reflectance.\n$$NDBI = \\frac{(SWIR - NIR)}{(SWIR + NIR)}$$\n\n\nSoil Adjusted Vegetation Index (SAVI)\nSimilar to NDVI but adjusts for soil background effects, particularly in areas with sparse vegetation cover.\n$$SAVI = \\frac{((NIR - Red) \\times (1 + L))}{(NIR + Red + L)}$$ where ( L ) is the soil adjustment factor.\n\n\nEnhanced Vegetation Index (EVI)\nAn enhanced version of NDVI that incorporates blue and red-edge bands, providing better sensitivity to high-density vegetation areas.\n$$EVI = G \\times \\frac{(NIR - Red)}{(NIR + C_1 \\times Red - C_2 \\times Blue + L)}$$ where G is the gain factor, C_1 and C_2 are coefficients, and L is the canopy background adjustment.\n\n\nSoil Moisture Index (SMI)\nUsed to estimate soil moisture content and soil wetness conditions in agricultural and hydrological studies.\n$$SMI = \\frac{(NIR + SWIR)}{2}$$\n\n\nLand Surface Temperature and Vegetation Index (LSTVI)\nCombines information from land surface temperature (LST) and vegetation indices to analyze urban heat island effects and land use changes.\n$$LSTVI = \\frac{(Tb - NDVI)}{(Tb + NDVI)}$$ where ( Tb ) is the land surface temperature.\n\n\n\n\n\n\n\nThese indices play crucial roles in various applications such as land cover classification, vegetation monitoring, and environmental assessments."
  },
  {
    "objectID": "wk3_RS_data.html#reflection",
    "href": "wk3_RS_data.html#reflection",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.4 Reflection",
    "text": "3.4 Reflection\nDuring my undergraduate dissertation project, I encountered challenges in processing remote sensing data due to significant smoke obscuring vegetation on the land surface. It was my first experience using Python to convert remote sensing images into multidimensional numerical matrices, where each dimension corresponded to different spectral bands of the original remote sensing data. Interestingly, the principle behind performing band calculations in Python was essentially the same as that of commonly used remote sensing index formulas. I realized that applying index calculation formulas in Python involved performing arithmetic operations such as addition, subtraction, multiplication, and division on these multidimensional numerical matrices.\nSo this is an example of the band processing I did in my undergraduate program to make the forest fire point more prominent in the image:\n\n\n\nComparison before and after band optimization\n\n\nThis realization underscores the significance of hands-on practice and theoretical knowledge integration in effectively utilizing remote sensing techniques for various environmental analyses and research endeavors."
  },
  {
    "objectID": "wk5_GEE.html#in-the-practical-exercises-section-i-learned",
    "href": "wk5_GEE.html#in-the-practical-exercises-section-i-learned",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.3 In the practical exercises section, I learned",
    "text": "5.3 In the practical exercises section, I learned\nAdvanced pixel-level image transformations: How to perform Principal Component Analysis (PCA) and Tasseled Cap transformations in GEE, which are methods for dimensionality reduction and feature extraction used for image classification and change detection in remote sensing imagery.\nGEE applications and data catalog: GEE can also create interactive visualization applications to showcase interesting and useful remote sensing analysis cases. GEE also provides a vast data catalog, including high-resolution satellite imagery, air pollution data, administrative boundaries, and other datasets."
  },
  {
    "objectID": "wk5_GEE.html#section",
    "href": "wk5_GEE.html#section",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.3 ",
    "text": "5.3"
  },
  {
    "objectID": "wk5_GEE.html#reflection",
    "href": "wk5_GEE.html#reflection",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.4 Reflection",
    "text": "5.4 Reflection\nReflecting on this week’s learning, I delved into the CASA0025: Building Spatial Applications with Big Data course, which shares similarities with our remote sensing syllabus!!! The collaboration between the two courses’ instructors is a testament to the interdisciplinary nature of these fields. It’s enlightening to see how knowledge from both domains can be integrated and enhance our understanding.\nOne aspect of GEE that stands out to me as particularly user-friendly, especially for engineers and scholars, is the accessibility of remote sensing data. Unlike proprietary data, GEE offers a wealth of openly available geospatial information. All that’s required is to pinpoint the specific time and location of interest and retrieve the corresponding API. This approach to data sharing is not only equitable but also fosters a spirit of community and collaboration within the field."
  },
  {
    "objectID": "wk4_policy.html#problem-context-background",
    "href": "wk4_policy.html#problem-context-background",
    "title": "4  Week4 - Policy Applications",
    "section": "4.1 Problem: Context & Background",
    "text": "4.1 Problem: Context & Background\nBeijing recently faced an unprecedented natural event: the heaviest rainfall in 140 years occurred between July 29 and August 1, 2023. This extreme weather event led to widespread flooding, impacting the city’s infrastructure, residents, and environment. Several factors contributed to the severity of the floods, including climate conditions, landform characteristics, and the effects of rapid urbanization."
  },
  {
    "objectID": "wk4_policy.html#policy-case-study",
    "href": "wk4_policy.html#policy-case-study",
    "title": "4  Week4 - Policy Applications",
    "section": "4.2 2. Policy & Case Study",
    "text": "4.2 2. Policy & Case Study\n\n4.2.1 2.1 UN Sustainable Development Goals\nIn response to such climate-related challenges, it is crucial to align efforts with the United Nations’ Sustainable Development Goals (SDGs). Key objectives include prioritizing wastewater treatment and sustainable urbanization. By integrating these goals into urban planning and development, cities like Beijing can enhance their resilience to climate disasters. Additionally, following the Sendai Framework for Disaster Risk Reduction provides a comprehensive approach to disaster preparedness and mitigation.\n\n\n4.2.2 2.2 Beijing Urban Resilience Strategy\nBeijing’s urban resilience strategy aims to address vulnerabilities exacerbated by urbanization. Key components include:\n\nUpgrading Emergency Disaster Prevention Technology: Investing in advanced monitoring systems, early warning mechanisms, and real-time data analytics can significantly enhance disaster preparedness and response.\nImproving Emergency Rescue Capability: Strengthening emergency services, training first responders, and establishing efficient evacuation protocols are essential for minimizing casualties during extreme events.\nFostering Cross-Sectoral Coordination and Collaboration: Effective disaster management requires collaboration among government agencies, private sectors, NGOs, and local communities. Coordinated efforts can enhance overall resilience and facilitate timely responses.\n\nBy implementing these strategies, Beijing can better withstand future climate-related challenges and protect its residents and infrastructure.\n\n\n\nConnect artificial or natural ponds and lakes with nearby rivers and create above-ground and groundwater systems for drainage and collecting excess water"
  },
  {
    "objectID": "wk4_policy.html#reference",
    "href": "wk4_policy.html#reference",
    "title": "4  Week4 - Policy Applications",
    "section": "4.5 Reference",
    "text": "4.5 Reference\nBeijing Municipal Emergency Committee.(2022).Notice of the Beijing Municipal Emergency Committee on Printing and Distributing the Beijing Flood Control Emergency Plan (Revised in 2022).https://www.beijing.gov.cn/zhengce/zhengcefagui/202308/t20230807_3216832.html\nDing, X., Liao, W., Lei, X., Wang, H., Yang, J., & Wang, H. (2022). Assessment of the impact of climate change on urban flooding: A case study of Beijing, China. Journal of Water and Climate Change, 13(10), 2692–3715. https://doi.org/10.2166/wcc.2022.224\nProposals for China’s Vision for 2035 (2023) Available at: https://www.mem.gov.cn/xw/ztzl/2020/xxgcwzqh/qwjd/zjjd/202012/t20201208_374880.shtml (Accessed: 10 March 2024)."
  },
  {
    "objectID": "wk6_classification1.html",
    "href": "wk6_classification1.html",
    "title": "6  Week7 - Classification-I",
    "section": "",
    "text": "6.1 Overfitting\n$$Tree score = SSR + tree penalty (alpha) * T (number of leaves)$$\nFor the Tree score formula, why does removing more leaves result in a larger alpha?\nDuring the pruning process of decision trees, we use a parameter called ccp_alpha (cost complexity parameter) to control the extent of pruning. A higher value of this parameter indicates a greater tendency to remove more leaf nodes, thereby simplifying the tree structure.\nLet’s understand why removing more leaf nodes leads to a larger ccp_alpha:\n6.1.1 Gini Impurity and Tree Complexity:\nGini impurity is a metric used to measure the purity of a dataset, with smaller values indicating higher purity.\nDuring pruning, we aim to retain leaf nodes that positively contribute to the model’s performance while reducing the complexity of the model.\nWhen we remove a leaf node, the Gini impurity increases because we lose the purity associated with that leaf node.\n6.1.2 Cost Complexity:\nccp_alpha is a cost complexity parameter that balances the fit and complexity of the model during pruning.\nBy increasing ccp_alpha, the model tends to remove more leaf nodes, thereby reducing the complexity of the model.\nThe purpose of this is to prevent overfitting and improve the generalization ability of the model."
  },
  {
    "objectID": "wk6_classification1.html#overfitting",
    "href": "wk6_classification1.html#overfitting",
    "title": "6  Week6 - Classification-I",
    "section": "6.1 Overfitting",
    "text": "6.1 Overfitting\n\\[Tree score = SSR + TreePenalty_{alpha}* T_{Mumber Of Leaves}\\]\nFor the Tree score formula, why does removing more leaves result in a larger alpha?\nDuring the pruning process of decision trees, we use a parameter called ccp_alpha (cost complexity parameter) to control the extent of pruning. A higher value of this parameter indicates a greater tendency to remove more leaf nodes, thereby simplifying the tree structure.\nLet’s understand why removing more leaf nodes leads to a larger ccp_alpha:\n\n6.1.1 Gini Impurity and Tree Complexity:\nGini impurity is a metric used to measure the purity of a dataset, with smaller values indicating higher purity.\nDuring pruning, we aim to retain leaf nodes that positively contribute to the model’s performance while reducing the complexity of the model.\nWhen we remove a leaf node, the Gini impurity increases because we lose the purity associated with that leaf node.\n\n\n\nRepresentation of a decision tree before and after being processed by a pruning algorithm, where the decision nodes (light blue) classify samples into 2 classes (green and red). The red dividing lines represent the pruning step.\n\n\n\n\n6.1.2 Cost Complexity:\nccp_alpha is a cost complexity parameter that balances the fit and complexity of the model during pruning.\nBy increasing ccp_alpha, the model tends to remove more leaf nodes, thereby reducing the complexity of the model.\nThe purpose of this is to prevent overfitting and improve the generalization ability of the model."
  },
  {
    "objectID": "wk9_SAR_data.html#section",
    "href": "wk9_SAR_data.html#section",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.3 ",
    "text": "8.3"
  },
  {
    "objectID": "wk9_SAR_data.html#sar-data-application",
    "href": "wk9_SAR_data.html#sar-data-application",
    "title": "8  Week9 - Synthetic Aperture Radar (SAR) data",
    "section": "8.4 SAR data application",
    "text": "8.4 SAR data application\nAnalyze the change between two images (e.g. ratio or logarithmic ratio)\nOpen Access Damage Detection Using Sentinel-1 Imagery\nBlast Damage Assessment\nSee the difference over time in the following ways:\nPixel-Wise T-Test"
  },
  {
    "objectID": "wk4_policy.html#beijing-sponge-city-strategy",
    "href": "wk4_policy.html#beijing-sponge-city-strategy",
    "title": "4  Week4 - Policy Applications",
    "section": "4.3 Beijing Sponge City Strategy",
    "text": "4.3 Beijing Sponge City Strategy\n\nThis standard aims to scientifically and rationally compile sponge city planning, ensuring effective roles in water safety, resources, environment, ecology, and culture.\n\nIssuing Bodies: Compiled by the Beijing Urban Planning and Design Research Institute and the China Urban Planning and Design Research Institute.\nApproval Departments: Approved by the Beijing Municipal Planning and Natural Resources Committee and the Beijing Municipal Market Supervision Administration.\nImplementation Date: Came into effect on January 1, 2021.\nContent Summary: Outlines requirements for compiling sponge city plans within Beijing’s administrative region, covering general urban planning, district planning, detailed planning, and town planning. Also includes assessment of sponge city planning implementation."
  },
  {
    "objectID": "wk4_policy.html#reflection",
    "href": "wk4_policy.html#reflection",
    "title": "4  Week4 - Policy Applications",
    "section": "4.4 Reflection",
    "text": "4.4 Reflection\nIn this course, I gained valuable insights from examining a city’s policies related to natural disaster protection. Among the new concepts I encountered, the most memorable one was the idea of a “sponge city.” This term vividly illustrates Beijing’s approach to mitigating flood hazards by implementing innovative measures.\nThe concept of a sponge city emphasizes the need to absorb and manage excess water during heavy rainfall, thereby reducing the risk of flooding. Beijing’s strategy involves creating permeable surfaces, enhancing green spaces, and improving drainage systems. Witnessing how a city like Beijing tackles flood challenges left a lasting impression on me.\nOne significant challenge in implementing urban policies is striking a balance between macro-level planning and local adjustments. How can a city retain its existing infrastructure while making targeted changes to enhance resilience? This delicate balance requires thoughtful consideration and cross-disciplinary collaboration.\nOverall, this course highlighted the complexities of urban resilience and the importance of forward-thinking policies in safeguarding cities against natural disasters. As I continue my studies, I’ll keep the lessons from this class in mind, especially the innovative approaches like sponge cities that can shape our urban future."
  },
  {
    "objectID": "wk5_GEE.html#application",
    "href": "wk5_GEE.html#application",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\nHere’s a summary of the Google Earth Engine (GEE) applications:\n\nRemote Sensing: GEE processes remote sensing data, useful for environmental and climate analysis.\nHistorical Imagery: It stores historical satellite imagery for geospatial analyses, like forest and water coverage.\nData Access: Users can access a vast database of pre-processed datasets and imagery for their own analysis1.\nReal-World Applications: GEE is applied in various fields, including vegetation analysis, land cover studies, and natural disaster management.\n\nThis picture provides an overview of how GEE can be utilized in environmental science, highlighting its capabilities and benefits.\n\n\n\nGEE applications (Meisam Amani 2020)"
  },
  {
    "objectID": "wk5_GEE.html#references",
    "href": "wk5_GEE.html#references",
    "title": "5  Week5 - Google Earth Engine",
    "section": "5.5 References",
    "text": "5.5 References\n• Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., & Moore, R. (2017). Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote Sensing of Environment.\n• Rafael Louzeiro – GIS/Environmental Specialist, Google Earth Engine – How it can be used in Environmental Science1, Jun 3, 2021, Integrate Sustainability Pty Ltd, https://www.integratesustainability.com.au/2021/06/03/google-earth-engine-how-it-can-be-used-in-environmental-science/\n• Meisam Amani, Senior Member, IEEE, Arsalan Ghorbanian , Seyed Ali Ahmadi , Mohammad Kakooei , Armin Moghimi , S. Mohammad Mirmazloumi, Student Member, IEEE, Sayyed Hamed Alizadeh Moghaddam , Sahel Mahdavi, Masoud Ghahremanloo, Saeid Parsian, Qiusheng Wu. 2020. Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review. IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 13,."
  },
  {
    "objectID": "wk8_SAR_data.html#abstract",
    "href": "wk8_SAR_data.html#abstract",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.1 Abstract",
    "text": "8.1 Abstract\nThis class introduces the basic principle of synthetic aperture radar (SAR), two sensor types, SAR data background, SAR data value, SAR polarization, SAR background, differential interferometric synthetic aperture radar (DlnSAR), SAR data processing, SAR data fusion, SAR image fusion and other concepts. This paper also introduces how to use SAR data for change detection, including statistical test, threshold screening and ROC curve, image fusion and so on. Finally, a change detection algorithm based on SAR data is proposed, and an example is given."
  },
  {
    "objectID": "wk8_SAR_data.html#difference-of-other-sar",
    "href": "wk8_SAR_data.html#difference-of-other-sar",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.2 Difference of other SAR",
    "text": "8.2 Difference of other SAR\n\n\n\n\n\n\n\n\n\nSAR\nInSAR\nDInSAR\nPSInSAR\n\n\n\n\nSynthetic Aperture Radar\nInterferometric Synthetic Aperture Radar\nDifferential Interferometric Synthetic Aperture Radar\nPersistent Scatterer Interferometric SAR\n\n\n\nA radar system capable of producing high-resolution images.\nUses a “virtual” antenna length to combine echo signals received from different positions, resulting in higher-resolution radar imaging.\nUsed for surface observations such as land use, topography, and forest cover.\n\n\nAnalyzes surface deformation by exploiting the phase difference between two remote sensing images.\nCalculates the deformation at each pixel on the ground surface between two observations.\nReveals elevation and deformation information.\n\n\nBuilds upon InSAR by using phase differences from multiple remote sensing images to improve deformation measurement accuracy.\nSensitive to deformation, suitable for monitoring ground surface changes due to earthquakes, mining, landslides, etc.\nUtilizes two SAR images and external topographic data to measure subtle surface deformations.\n\n\nModels and analyzes time series data from multiple SAR images to enhance deformation inversion accuracy.\nReveals spatial distribution of surface deformations, widely used for monitoring urban subsidence and infrastructure changes."
  },
  {
    "objectID": "wk8_SAR_data.html#section",
    "href": "wk8_SAR_data.html#section",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.3 ",
    "text": "8.3"
  },
  {
    "objectID": "wk8_SAR_data.html#sar-data-application",
    "href": "wk8_SAR_data.html#sar-data-application",
    "title": "8  Week8 - Synthetic Aperture Radar (SAR) data",
    "section": "8.3 SAR data application",
    "text": "8.3 SAR data application\nAnalyze the change between two images (e.g. ratio or logarithmic ratio)\nOpen Access Damage Detection Using Sentinel-1 Imagery\nBlast Damage Assessment\nSee the difference over time in the following ways:\nPixel-Wise T-Test"
  },
  {
    "objectID": "wk6_classification1.html#reflection",
    "href": "wk6_classification1.html#reflection",
    "title": "6  Week6 - Classification-I",
    "section": "6.2 Reflection",
    "text": "6.2 Reflection\nIn the field of remote sensing, these concepts pose several challenges and considerations that I find intriguing. Ensuring that the models I develop have strong generalization capabilities, capable of adapting to various environmental conditions, is paramount. Additionally, assessing and addressing data impurities arising from diverse factors in remote sensing datasets presents a fascinating puzzle to solve. Moreover, navigating the balance between constructing complex models to capture nuanced data variations while avoiding overfitting is a stimulating challenge that I look forward to tackling in this course."
  },
  {
    "objectID": "wk7_classification2.html#reflection",
    "href": "wk7_classification2.html#reflection",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.2 Reflection",
    "text": "7.2 Reflection\nThis reminds me of my undergraduate dissertation project, where I evaluated the accuracy of fire detection models using confusion matrices.\n\n\n\nConfusion Matrix\n\n\nFirstly, I prepared the data by creating binary classification labels using both active detection thresholding and visual inspection methods as ground truth data. Next, I trained the model using a U-net neural network model on remote sensing images, applying both cross-entropy and focal loss functions. Finally, I made predictions and evaluations by predicting fire pixel locations in new images and assessing the differences between expected and original images using confusion matrix evaluation metrics to determine prediction accuracy. Evaluating accuracy was a crucial practical experience for me in using convolutional networks to predict fire spots in remote sensing images."
  },
  {
    "objectID": "intro.html#application",
    "href": "intro.html#application",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.3 Application",
    "text": "1.3 Application\n\n1.3.1 Remote Sensing Bands and Environmental Applications\nThis section explores how different spectral bands used in remote sensing contribute to monitoring and managing environmental phenomena such as forest vegetation, water bodies, and soil classification. Each application highlights specific bands that enhance detection accuracy and provide critical insights for environmental studies and management.\n\n\n\nEnvironmental Application\nExample\n\n\n\n\nForest Vegetation\n\n\nSensitive Bands:\nRed, Near-Infrared (NIR)\nDeforestation Monitoring\n\nClearcuts in radar images (white boxes). The radar backscatter differences in the SAR image allow to distinguish between forest and non-forest areas and make it possible to map and measure the extent of deforestation.\n\n\nFor change detection analysis, a stack of radar images is needed. Figure 4 shows an example for ongoing deforestation (white polygons) in the Guaviare Department, Colombia, between January and May 2020.\n\n\n\nWater Bodies\n\nSensitive Bands: Blue, Green\nDevendra Singh et al. (2023)2 have leveraged geospatial modeling systems and advanced spatial and spectral resolution sensors to monitor various factors related to water quality at an affordable cost and with higher accuracy, including turbidity, chlorophyll-a, suspended residues, and algal blooms in different water bodies. This chapter provides a detailed review of the role of Geographic Information Systems (GIS) and remote sensing in the monitoring, management, and remediation of water quality.\n\n\nSoil Classification\n\nSensitive Bands: Red, Shortwave Infrared (SWIR)\nHyperspectral remote sensing (HRS) (Huan Yu et al, 2020)3 is used for a detailed analysis of soil spectral properties, with extensive research over the past 40 years. HRS helps identify various soil properties such as minerals, nutrients, organic carbon, moisture, salinity, and texture."
  },
  {
    "objectID": "intro.html#summary",
    "href": "intro.html#summary",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nBefore starting this course, I need to first understand the purpose of this course, which is to introduce the basic concepts and applications of remote sensing, and through this course of RS, the content we will learn includes:\n\n\n\n\n\n\nSummary of the first Class\n\n\n\n\nTele-sensing Cities and Environments: This is a course that introduces the basic principles and applications of tele-sensing, with a focus on urban and environmental issues.\nTypes and principles of remote sensing: Remote sensing is divided into active and passive, depending on whether it has its own energy transmitter. Remote sensing data is affected by the interaction of electromagnetic waves with the atmosphere and the surface, and needs to be corrected and processed.\nFormat and resolution of remote sensing data: Remote sensing data is usually raster data and has different storage formats and ways of organizing. The quality and application of remote sensing data are constrained by spatial, spectral, temporal and radiative resolution.\n\n\n\nIt easily reminds me of the basic concepts of RS that I learned by myself during my undergraduate study. I believe that this course can not only help me review the past knowledge, but also fully supplement other knowledge."
  },
  {
    "objectID": "intro.html#application2",
    "href": "intro.html#application2",
    "title": "1  Week1 - Introduction of RS",
    "section": "1.4 Application2",
    "text": "1.4 Application2\n\n1.4.1 Remote Sensing Bands and Environmental Applications\n\n\n\nEnvironmental Application\nSensitive Bands\nExample\n\n\n\n\nForest Vegetation\nRed, Near-Infrared (NIR)\nDeforestation Monitoring\n\nClearcuts in radar images (white boxes). The radar backscatter differences in the SAR image allow to distinguish between forest and non-forest areas and make it possible to map and measure the extent of deforestation.\n\n\nFor change detection analysis, a stack of radar images is needed. Figure 4 shows an example for ongoing deforestation (white polygons) in the Guaviare Department, Colombia, between January and May 2020.\n\n\n\nWater Bodies\nBlue, Green\nLow reflectance in water but higher in suspended sediments and algae. Used for water quality.\n\n\nSoil Classification\nRed, Shortwave Infrared (SWIR)\nDifferentiates soil types based on moisture content and mineral composition.\n\n\nFire Detection\nMid-Infrared (MIR), Thermal IR (TIR)\nDetects active fires and monitors post-fire surface temperature changes.\n\n\n\n\n\n1.4.2 Summary\n\n1.4.2.1 Forest Vegetation\n\nSensitive Bands: Red and Near-Infrared (NIR)\nBand Combination: Red/NIR\nDescription: High reflectance in NIR and low in Red for healthy vegetation. Used in NDVI calculation.\n\n\n\n1.4.2.2 Water Bodies\n\nSensitive Bands: Blue and Green\nBand Combination: Blue/Green\nDescription: Low reflectance in water but higher in suspended sediments and algae. Used for water quality monitoring.\n\n\n\n1.4.2.3 Soil Classification\n\nSensitive Bands: Red and Shortwave Infrared (SWIR)\nBand Combination: Red/SWIR\nDescription: Differentiates soil types based on moisture content and mineral composition.\n\n\n\n1.4.2.4 Fire Detection\n\nSensitive Bands: Mid-Infrared (MIR) and Thermal Infrared (TIR)\nBand Combination: MIR/TIR\nDescription: Detects active fires and monitors post-fire surface temperature changes."
  },
  {
    "objectID": "wk3_RS_data.html#application",
    "href": "wk3_RS_data.html#application",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.3 Application",
    "text": "3.3 Application\nIn practical applications, these indices are not used in isolation but are often combined to enhance the interpretation and extraction of information from imagery data. Combining multiple indices in remote sensing analysis can provide more comprehensive insights and improve the accuracy of the results.\nKim, S.W et al.(2021)3, discusses various indices used to estimate and analyze urban heat island (UHI) intensity and magnitude. It highlights the use of NDVI for extracting land surface temperature data from satellite images, NDBI for analyzing the built environment, EVI for assessing vegetation coverage, and the Biophysical Composition Index (BCI) for describing urban biophysical characteristics. These indices help researchers to more accurately estimate and analyze UHI intensity and magnitude.\n\n\n\n\n\n\n\nSchultz, M. et al.(2016)[^wk3_rs_data-4] evaluates the performance of eight vegetation indices (VI) from Landsat time series (LTS) for monitoring deforestation across tropical regions. It uses a robust reference database to assess spatial accuracy, sensitivity to observation frequency, and the performance of combined VI. The study mentions the use of the Normalized Difference Fraction Index (NDFI) sensitive to canopy cover, moisture-related VIs (Normalized Difference Moisture Index (NDMI) and Tasseled Cap Wetness (TCw)) that performed spatially better than greenness-related VIs (Normalized Difference Vegetation Index (NDVI) and Tasseled Cap Greenness (TCg)). The spatial accuracy improved, and the overestimation of changes decreased when VIs were fused at the feature level. [^wk3_rs_data-4]: Schultz, M., Verbesselt, J.G.P.W., Carter, S., Verbesselt, J., Avitabile, V., Quang, H.V., & Herold, M. (2016). ‘Performance of Landsat time series vegetation indices for monitoring deforestation’, _International Journal of Applied Earth Observation and Geoinformation_, vol. 52, pp. 318-327.\n\n\n\n\nThese examples illustrate how combining multiple indices can enhance the analysis and provide more accurate and detailed information for various remote sensing applications."
  },
  {
    "objectID": "wk3_RS_data.html#footnotes",
    "href": "wk3_RS_data.html#footnotes",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "",
    "text": "Lillesand, T.M. & Kiefer, R.W., 1987. Remote sensing and image interpretation. 2nd ed1. New York: Wiley. https://archive.org/details/remotesensingima00lill/page/1/mode/1up↩︎\nTempfli, K, Huurneman, GC, Bakker, WH, Janssen, LLF, Feringa, WF, Gieske, ASM, Grabmaier, KA, Hecker, CA, Horn, JA, Kerle, N, van der Meer, FD, Parodi, GN, Pohl, C, Reeves, CV, van Ruitenbeek, FJA, Schetselaar, EM, Weir, MJC, Westinga, E & Woldai, T 2009, Principles of remote sensing: an introductory textbook. ITC Educational Textbook Series, vol. 2, International Institute for Geo-Information Science and Earth Observation, Enschede. http://www.itc.nl/library/papers_2009/general/PrinciplesRemoteSensing.pdf↩︎\nKim, S.W. and Brown, R., 2021. “Urban Heat Island (UHI) intensity and magnitude estimation: A systematic literature review.” Science of The Total Environment, 779, p.146389. DOI: &lt;10.1016/j.scitotenv.2021.146389&gt;↩︎"
  },
  {
    "objectID": "wk3_RS_data.html#summary",
    "href": "wk3_RS_data.html#summary",
    "title": "3  Week3 - Remote Sensing Data",
    "section": "3.2 Summary",
    "text": "3.2 Summary\nThe various techniques, including atmospheric correction and imagery merging, used in remote sensing to enhance and process imagery data share common principles. While they serve different purposes1, they share some common principles:\n\nData Transformation: Most of these techniques involve transforming the raw data to enhance its usability and interpretability. This transformation can be radiometric, spatial, or spectral.\nEnhancement of Information: These methods aim to enhance the quality of the data to make it more useful for analysis. This could involve reducing noise, highlighting certain features, or combining data from different sources.\nMathematical and Statistical Methods: Many of these techniques rely on mathematical and statistical methods to process the data. This includes algorithms for filtering, statistical analysis for PCA, and mathematical models for atmospheric correction.\nImprovement of Visualization: Techniques such as enhancement and filtering are often used to improve the visualization of imagery, making it easier to interpret and analyze.\n\n\n\n\n\n\nRemote sensing process and ‘Remote Sensing for Earth Observation’ book chapters(Klaus Temfli et al., 2009)2\nIn remote sensing analysis, the Normalized Difference Vegetation Index (NDVI) is a widely used index for assessing vegetation health and density by comparing the difference between near-infrared (NIR) and red band reflectance. \\[NDVI= \\frac{NIR-Red}{NIR+Red}\\]\nBesides NDVI, various other indices are commonly used to extract information about land cover and land use, enhancing the overall information derived from remote sensing data. Here is a summary of some common remote sensing indices.\n\n\n\nSummary of Common Remote Sensing Indices\n\n\nIndex\nDescription\nFormula\n\n\n\n\nNormalized Difference Water Index (NDWI)\nUtilized to identify water bodies by comparing the difference between green and near-infrared band reflectance.\n$$NDWI = \\frac{(Green - NIR)}{(Green + NIR)}$$\n\n\nNormalized Difference Built-up Index (NDBI)\nEmployed to detect built-up areas by comparing the difference between near-infrared and shortwave infrared band reflectance.\n$$NDBI = \\frac{(SWIR - NIR)}{(SWIR + NIR)}$$\n\n\nSoil Adjusted Vegetation Index (SAVI)\nSimilar to NDVI but adjusts for soil background effects, particularly in areas with sparse vegetation cover.\n$$SAVI = \\frac{((NIR - Red) \\times (1 + L))}{(NIR + Red + L)}$$ where ( L ) is the soil adjustment factor.\n\n\nEnhanced Vegetation Index (EVI)\nAn enhanced version of NDVI that incorporates blue and red-edge bands, providing better sensitivity to high-density vegetation areas.\n$$EVI = G \\times \\frac{(NIR - Red)}{(NIR + C_1 \\times Red - C_2 \\times Blue + L)}$$ where G is the gain factor, C_1 and C_2 are coefficients, and L is the canopy background adjustment.\n\n\nSoil Moisture Index (SMI)\nUsed to estimate soil moisture content and soil wetness conditions in agricultural and hydrological studies.\n$$SMI = \\frac{(NIR + SWIR)}{2}$$\n\n\nLand Surface Temperature and Vegetation Index (LSTVI)\nCombines information from land surface temperature (LST) and vegetation indices to analyze urban heat island effects and land use changes.\n$$LSTVI = \\frac{(Tb - NDVI)}{(Tb + NDVI)}$$ where ( Tb ) is the land surface temperature.\n\n\n\n\n\n\n\nThese indices play crucial roles in various applications such as land cover classification, vegetation monitoring, and environmental assessments."
  }
]